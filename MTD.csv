Numbers,Words,Definitions,Citations,Naturals
0,Artificial Intelligence,A machine with the ability to replicate cognitive activities associated with human thought.,"The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).","Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals."
1,Natural Language Processing,The study of linguistics from a mathematical or computational perspective.,"Noam Chomsky, Syntactic Structures, 11 (Mouton & Co 1957).","Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
2,Deep Learning,A sub-set of machine learning focusing on multi-layered neural networks with the goal of identifying associative properties.,"Ethem Alapaydin, Machine Learning, 99 (The MIT Press, 2016).","Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised."
3,Gradient Descent,An iterative optimization algorithm for finding the minimum value of a function.,"Alex Kendall, et. al., Learning to Drive in A Day, 2 (2018) https://arxiv.org/abs/1807.00412. ","Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."
4,Reward Function,Represents the expected reward received for an action in a given state.,"Haney, Brian Seamus, The Optimal Agent: The Future of Autonomous Vehicles & Liability Theory (October 5, 2018). Available at SSRN: https://ssrn.com/abstract=3261275 or http://dx.doi.org/10.2139/ssrn.3261275",The environment moves to a new state and the reward associated with the transition is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can (possibly randomly) choose any action as a function of the history.
5,Superintelligence,Intelligence far beyond human level in all domains.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","Superintelligence: Paths, Dangers, Strategies is a 2014 book by the Swedish philosopher Nick Bostrom from the University of Oxford. It argues that if machine brains surpass human brains in general intelligence, then this new superintelligence could replace humans as the dominant lifeform on Earth. "
6,Artificial General Intelligence,AI with the ability to accomplish any goal or learn how to perform any task.,"Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence, 39 (Penguin Random House 2017).",Artificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies.
7,Value Function,Function defining the positive outcome for an agent.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction, 92 (MIT Press 2017).","A value function is not a ""return function"", it is an ""expected return function"" and that is an important difference. A return is a measured value (or a random variable, when discussed in the abstract) representing the actual [discounted] sum of rewards seen following a specific state or state/action pair."
8,Narrow Artificial Intelligence,"Artificial Intelligence that has the ability to accomplish a narrow set of goals, like drive a car or play chess.","Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence (Penguin Random House 2017).","Weak artificial intelligence (weak AI), also known as narrow AI is artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to either strong AI (a machine with consciousness, sentience and mind) or artificial general intelligence (a machine with the ability to apply intelligence to any problem, rather than just one specific problem). Many currently existing systems that claim to use ""artificial intelligence"" are likely operating as a weak AI focused on a narrowly defined specific problem."
9,Policy Optimization,The process of developing an optimal decision making strategy.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","The theory of MDPs states that if is an optimal policy, we act optimally (take the optimal action) by choosing the action from with the highest value at each state, . The action-value function of such an optimal policy ( ) is called the optimal action-value function."
10,Unsupervised Learning,Refers to machine learning where algorithms infer structures in unlabeled data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data."
11,Reinforcement Learning,Process of discovering maximum reward for sequantial decision making.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","Reinforcement Learning is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. It allows machines and software agents to automatically determine the ideal behaviour within a specific context, in order to maximize its performance."
12,Supervised Learning,A subfield of machine learning where a human trains a neural network to learn from labeled data sets.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Supervised learning is the Data mining task of inferring a function from labeled training data.The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called thesupervisory signal)."
13,Artificial Neuron,A logic gate modeling the biological neuron.,"Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence (Penguin Random House 2017).","An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network."
14,Neural Network,"A neural network is a function, or transformation of information, operating on input data allowing an inference of abstract meaning from the corresponding output.","The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).",a computer system modeled on the human brain and nervous system.
15,Recurrent Neural Network,A type of neural network where the neurons have connections to neurons in the same or preceding layers allowing for a form of short term memory.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).",A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a sequence.
16,Markov Process,A mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. The agent chooses an action at a time based on observing the state.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty, 77 (MIT Press 2015).","A Markov process is a random process in which the future is independent of the past, given the present. Thus, Markov processes are the natural stochastic analogs of the deterministic processes described by differential and difference equations. They form one of the most important classes of random processes."
17,Q-Learning,"A reinforcement learning technique. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. For any finite Markov decision process, Q-learning finds an optimal policy, maximizing the expected value of the total reward.","Mykel J. Kochenderfer, Decision Making Under Uncertainty, 77 (MIT Press 2015).","Q-learning is a reinforcement learning technique used in machine learning. The goal of Q-learning is to learn a policy, which tells an agent what action to take."
18,Parameter,Variables passed to a function.,"Python, Documentation, Glossary (2018) https://docs.python.org/2/glossary.html#term-parameter.","Parameters and arguments. ... The term parameter (sometimes called formal parameter) is often used to refer to the variable as found in the function definition, while argument (sometimes called actual parameter) refers to the actual input supplied at function call."
19,Method,Behaviors of an object.,"Python, Documentation, Glossary (2018) https://docs.python.org/2/glossary.html#term-parameter.","A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object is mostly made up of data and behavior, which form the interface that an object presents to the outside world. Data is represented as properties of the object and behavior as methods."
20,TensorFlow,Open source software library used for machine learning and dataflow programming.,"TensorFlow, Learn (2018) https://www.tensorflow.org/tutorials/?nav=true.","TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud. See the sections below to get started."
21,OpenAI Gym,Toolkit written in python for developing reinforcement learning agents with an interface and a wide variety of environments.,"OpenAI, Gym (2018) https://gym.openai.com/.","Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
22,Markov Assumption,The next state depends only on the current state and action and not on any prior state or action.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model. A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items."
23,Bayseian Network,A type of probabilistic model with nodes corresponding to random variables.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
24,Chain Rule,Specifies how to construct a joint probability distribution from the local conditional probability distributions.,"Hyrum S. Anderson, et.al., Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning, Cornell University Library, 2 (January 30, 2018).","In probability theory, the chain rule (also called the general product rule[1][2]) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities."
25,Dynamic Programming,"A method for solving complex problems by breaking them down into a collection of simpler sub-problems, solving each of those sub-problems just once and storing the solutions.","Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation."
26,Temporal Difference learning,A model-free approach to reinforcement learning.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods. While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known."
27,Discount Factor,A number typically defined between 0 and 1 allowing a utility function to be defined in finite terms.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).",The meaning of discount factor on reinforcement learning. Where is a cumulative score value and is the score value for the action choose.
28,Sigmoid Function,A function having an S shaped or sigmoidal curve.,"John D. Kelleher, Brenden Tierney, Data Science, (The MIT Press, 2018).","A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure."
29,Covariance,The property of a function of retaining its form when the variables are linearly transformed.,"Ioffe, S., and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167.","In probability theory and statistics, covariance is a measure of the joint variability of two random variables.[1] If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive.[2] In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation."
30,Eigenvector,"In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to.","Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","Eigenvectors are a special set of vectors associated with a linear system of equations (i.e., a matrix equation) that are sometimes also known as characteristic vectors, proper vectors, or latent vectors (Marcus and Minc 1988, p. 144). The determination of the eigenvectors and eigenvalues of a system is extremely important in physics and engineering, where it is equivalent to matrix diagonalization and arises in such common applications as stability analysis, the physics of rotating bodies, and small oscillations of vibrating systems, to name only a few. Each eigenvector is paired with a corresponding so-called eigenvalue. Mathematically, two different kinds of eigenvectors need to be distinguished: left eigenvectors and right eigenvectors. However, for many problems in physics and engineering, it is sufficient to consider only right eigenvectors. The term ""eigenvector"" used without qualification in such applications can therefore be understood to refer to a right eigenvector."
