Numbers,Words,Definitions,Citations,Naturals
0,Artificial Intelligence,A machine with the ability to replicate cognitive activities associated with human thought.,"The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).","Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals."
1,Natural Language Processing,The study of linguistics from a mathematical or computational perspective.,"Noam Chomsky, Syntactic Structures, 11 (Mouton & Co 1957).","Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
2,Deep Learning,A sub-set of machine learning focusing on multi-layered neural networks with the goal of identifying associative properties.,"Ethem Alapaydin, Machine Learning, 99 (The MIT Press, 2016).","Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised."
3,Gradient Descent,An iterative optimization algorithm for finding the minimum value of a function.,"Alex Kendall, et. al., Learning to Drive in A Day, 2 (2018) https://arxiv.org/abs/1807.00412. ","Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."
4,Reward Function,Represents the expected reward received for an action in a given state.,"Haney, Brian Seamus, The Optimal Agent: The Future of Autonomous Vehicles & Liability Theory (October 5, 2018). Available at SSRN: https://ssrn.com/abstract=3261275 or http://dx.doi.org/10.2139/ssrn.3261275",The environment moves to a new state and the reward associated with the transition is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can (possibly randomly) choose any action as a function of the history.
5,Superintelligence,Intelligence far beyond human level in all domains.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","Superintelligence: Paths, Dangers, Strategies is a 2014 book by the Swedish philosopher Nick Bostrom from the University of Oxford. It argues that if machine brains surpass human brains in general intelligence, then this new superintelligence could replace humans as the dominant lifeform on Earth. "
6,Artificial General Intelligence,AI with the ability to accomplish any goal or learn how to perform any task.,"Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence, 39 (Penguin Random House 2017).",Artificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies.
7,Value Function,Function defining the positive outcome for an agent.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction, 92 (MIT Press 2017).","A value function is not a ""return function"", it is an ""expected return function"" and that is an important difference. A return is a measured value (or a random variable, when discussed in the abstract) representing the actual [discounted] sum of rewards seen following a specific state or state/action pair."
8,Narrow Artificial Intelligence,"Artificial Intelligence that has the ability to accomplish a narrow set of goals, like drive a car or play chess.","Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence (Penguin Random House 2017).","Weak artificial intelligence (weak AI), also known as narrow AI is artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to either strong AI (a machine with consciousness, sentience and mind) or artificial general intelligence (a machine with the ability to apply intelligence to any problem, rather than just one specific problem). Many currently existing systems that claim to use ""artificial intelligence"" are likely operating as a weak AI focused on a narrowly defined specific problem."
9,Policy Optimization,The process of developing an optimal decision making strategy.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","The theory of MDPs states that if is an optimal policy, we act optimally (take the optimal action) by choosing the action from with the highest value at each state, . The action-value function of such an optimal policy ( ) is called the optimal action-value function."
10,Unsupervised Learning,Refers to machine learning where algorithms infer structures in unlabeled data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data."
11,Reinforcement Learning,Process of discovering maximum reward for sequantial decision making.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","Reinforcement Learning is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. It allows machines and software agents to automatically determine the ideal behaviour within a specific context, in order to maximize its performance."
12,Supervised Learning,A subfield of machine learning where a human trains a neural network to learn from labeled data sets.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Supervised learning is the Data mining task of inferring a function from labeled training data.The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called thesupervisory signal)."
13,Artificial Neuron,A logic gate modeling the biological neuron.,"Max Tegmark, Life 3.0 Being Human in The Age of Artificial Intelligence (Penguin Random House 2017).","An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network."
14,Neural Network,"A neural network is a function, or transformation of information, operating on input data allowing an inference of abstract meaning from the corresponding output.","The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).",a computer system modeled on the human brain and nervous system.
15,Recurrent Neural Network,A type of neural network where the neurons have connections to neurons in the same or preceding layers allowing for a form of short term memory.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).",A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a sequence.
16,Markov Process,A mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. The agent chooses an action at a time based on observing the state.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty, 77 (MIT Press 2015).","A Markov process is a random process in which the future is independent of the past, given the present. Thus, Markov processes are the natural stochastic analogs of the deterministic processes described by differential and difference equations. They form one of the most important classes of random processes."
17,Q-Learning,"A reinforcement learning technique. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. For any finite Markov decision process, Q-learning finds an optimal policy, maximizing the expected value of the total reward.","Mykel J. Kochenderfer, Decision Making Under Uncertainty, 77 (MIT Press 2015).","Q-learning is a reinforcement learning technique used in machine learning. The goal of Q-learning is to learn a policy, which tells an agent what action to take."
18,Parameter,Variables passed to a function.,"Python, Documentation, Glossary (2018) https://docs.python.org/2/glossary.html#term-parameter.","Parameters and arguments. ... The term parameter (sometimes called formal parameter) is often used to refer to the variable as found in the function definition, while argument (sometimes called actual parameter) refers to the actual input supplied at function call."
19,Method,Behaviors of an object.,"Python, Documentation, Glossary (2018) https://docs.python.org/2/glossary.html#term-parameter.","A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object is mostly made up of data and behavior, which form the interface that an object presents to the outside world. Data is represented as properties of the object and behavior as methods."
20,TensorFlow,Open source software library used for machine learning and dataflow programming.,"TensorFlow, Learn (2018) https://www.tensorflow.org/tutorials/?nav=true.","TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud. See the sections below to get started."
21,OpenAI Gym,Toolkit written in python for developing reinforcement learning agents with an interface and a wide variety of environments.,"OpenAI, Gym (2018) https://gym.openai.com/.","Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
22,Markov Assumption,The next state depends only on the current state and action and not on any prior state or action.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model. A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items."
23,Bayseian Network,A type of probabilistic model with nodes corresponding to random variables.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
24,Chain Rule,Specifies how to construct a joint probability distribution from the local conditional probability distributions.,"Hyrum S. Anderson, et.al., Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning, Cornell University Library, 2 (January 30, 2018).","In probability theory, the chain rule (also called the general product rule[1][2]) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities."
25,Dynamic Programming,"A method for solving complex problems by breaking them down into a collection of simpler sub-problems, solving each of those sub-problems just once and storing the solutions.","Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation."
26,Temporal Difference learning,A model-free approach to reinforcement learning.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods. While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known."
27,Discount Factor,A number typically defined between 0 and 1 allowing a utility function to be defined in finite terms.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).",The meaning of discount factor on reinforcement learning. Where is a cumulative score value and is the score value for the action choose.
28,Sigmoid Function,A function having an S shaped or sigmoidal curve.,"John D. Kelleher, Brenden Tierney, Data Science, (The MIT Press, 2018).","A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure."
29,Covariance,The property of a function of retaining its form when the variables are linearly transformed.,"Ioffe, S., and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167.","In probability theory and statistics, covariance is a measure of the joint variability of two random variables.[1] If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive.[2] In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation."
30,Eigenvector,"In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to.","Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","Eigenvectors are a special set of vectors associated with a linear system of equations (i.e., a matrix equation) that are sometimes also known as characteristic vectors, proper vectors, or latent vectors (Marcus and Minc 1988, p. 144). The determination of the eigenvectors and eigenvalues of a system is extremely important in physics and engineering, where it is equivalent to matrix diagonalization and arises in such common applications as stability analysis, the physics of rotating bodies, and small oscillations of vibrating systems, to name only a few. Each eigenvector is paired with a corresponding so-called eigenvalue. Mathematically, two different kinds of eigenvectors need to be distinguished: left eigenvectors and right eigenvectors. However, for many problems in physics and engineering, it is sufficient to consider only right eigenvectors. The term ""eigenvector"" used without qualification in such applications can therefore be understood to refer to a right eigenvector."
31,Path Integral Formulation,Generalizes the action theory of classical mechanics.,"R.P. Feynman and A.R. Hibbs, Quantum Mechanics and Path Integrals (McGrawHill, New York, 1965).","Replaces the classical notion of a single, unique trajectory for a system with a sum or function integral, over an infinity of quantum mechanically possible trajectories to compute quantum amplitude."
32,Hamiltonian,Characterizes the total energy of the system under consideration.,"Perpelitsa, Dennis V., Path Integrals in Quantum Mechanics, MIT Department of Physics, http://web.mit.edu/dvp/www/Work/8.06/dvp-8.06-paper.pdf.","This Hamiltonian formulation works in many cases. In classical mechanics, however, the Lagrangian formulation is known to be equivalent to the Hamiltonian one. Thus, we seek an answer to the above question that relies on some analogue of the Lagrangian action."
33,Quantum Mechanics,The branch of mechanics dealing with mathematical descriptions of the motion and interaction of subatomic particles.,"R.P. Feynman and A.R. Hibbs, Quantum Mechanics and Path Integrals (McGrawHill, New York, 1965).","Quantum mechanics (QM; also known as quantum physics, quantum theory, the wave mechanical model, or matrix mechanics), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles. Classical physics, the physics existing before quantum mechanics, describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) Quantum mechanics differs from classical physics in that energy, momentum, angular momentum and other quantities of a bound system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave-particle duality); and there are limits to the precision with which quantities can be measured (uncertainty principle). Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA."
34,Schrödinger Equation,Mathematical equation describing changes over time of a physical system in which quantum effects are significant.,"Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","In quantum mechanics, the Schrödinger equation is a mathematical equation that describes the changes over time of a physical system in which quantum effects, such as wave–particle duality, are significant. These systems are referred to as quantum (mechanical) systems. The equation is considered a central result in the study of quantum systems, and its derivation was a significant landmark in the development of the theory of quantum mechanics. It was named after Erwin Schrödinger, who derived the equation in 1925, and published it in 1926, forming the basis for his work that resulted in him being awarded the Nobel Prize in Physics in 1933. In classical mechanics, Newton's second law (F = ma) is used to make a mathematical prediction as to what path a given physical system will take over time following a set of known initial conditions. Solving this equation gives the position, and the momentum of the physical system as a function of the external force F on the system. Those two parameters are sufficient to describe its state at each time instant. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). The equation is mathematically described as a linear partial differential equation, which describes the time-evolution of the system's wave function (also called a ""state function""). The concept of a wave function is a fundamental postulate of quantum mechanics, that defines the state of the system at each spatial position, and time. Using these postulates, Schrödinger's equation can be derived from the fact that the time-evolution operator must be unitary, and must therefore be generated by the exponential of a self-adjoint operator, which is the quantum Hamiltonian. This derivation is explained below. In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe. Schrödinger's equation is central to all applications of quantum mechanics including quantum field theory which combines special relativity with quantum mechanics. Theories of quantum gravity, such as string theory, also do not modify Schrödinger's equation. The Schrödinger equation is not the only way to study quantum mechanical systems and make predictions, as there are other quantum mechanical formulations such as matrix mechanics, introduced by Werner Heisenberg, and path integral formulation, developed chiefly by Richard Feynman. Paul Dirac incorporated matrix mechanics and the Schrödinger equation into a single formulation."
35,Newton’s Second Law,Determines a mathematical prediction as to what path a given physical system will take over time following a set of known initial conditions.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Solving this equation gives the position, and the momentum of the physical system as a function of the external force F on the system. Those two parameters are sufficient to describe its state at each time instant."
36,Eigenvalue,Each of a set of values of a parameter for which a differential equation has a nonzero solution under given conditions.,"Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v."
37,Stack,Data Structure used to store a collection of objects.,"Bradley N. Miller, David L. Ranum, Problem Solving with Algorithms and Data Structures Using Python (2nd. Franklin, Beedle & Associates 2011).","The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack. The name ""stack"" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first. Considered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack."
38,Word Embedding,A distributional Vector generally flowing from the distributional hypothesis.,"Tom Young et. al., Recent Trends in Deep Learning Based Natural Language Processing, Cornell University Library (Computational Intelligence Magazine 2018) https://arxiv.org/abs/1708.02709v5.","Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension. Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
39,Distributional Hypothesis,Words with similar meanings tend to occur in similar context.,"Tom Young et. al., Recent Trends in Deep Learning Based Natural Language Processing, Cornell University Library (Computational Intelligence Magazine 2018) https://arxiv.org/abs/1708.02709v5.","The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings. The underlying idea that ""a word is characterized by the company it keeps"" was popularized by Firth. The Distributional Hypothesis is the basis for statistical semantics. Although the Distributional Hypothesis originated in linguistics, it is now receiving attention in cognitive science especially regarding the context of word use. In recent years, the distributional hypothesis has provided the basis for the theory of similarity-based generalization in language learning: the idea that children can figure out how to use words they've rarely encountered before by generalizing about their use from distributions of similar words. The distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts. Whether or not this suggestion holds has significant implications for both the data-sparsity problem in computational modeling, and for the question of how children are able to learn language so rapidly given relatively impoverished input (this is also known as the problem of the poverty of the stimulus)."
40,Overfitting,"Describes a situation where a machine learning model captures the patterns in the training data well, but fails to generalize well to unseen data.","Sebastian Raschka, Vahid Mirjalili, Python Machine Learning (Packt 2017).","In statistics, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably"". An overfitted model is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure."
41,Black-Scholes Model,A mathematical model for the dynamics of a financial market containing derivative investment instruments.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","Black–Scholes–Merton model is a mathematical model for the dynamics of a financial market containing derivative investment instruments. From the partial differential equation in the model, known as the Black–Scholes equation, one can deduce the Black–Scholes formula, which gives a theoretical estimate of the price of European-style options and shows that the option has a unique price regardless of the risk of the security and its expected return (instead replacing the security's expected return with the risk-neutral rate). The formula led to a boom in options trading and provided mathematical legitimacy to the activities of the Chicago Board Options Exchange and other options markets around the world. It is widely used, although often with adjustments and corrections, by options market participants. Based on works previously developed by market researchers and practitioners, such as Louis Bachelier, Sheen Kassouf and Ed Thorp among others, Fischer Black and Myron Scholes demonstrated in the late 1960s that a dynamic revision of a portfolio removes the expected return of the security, thus inventing the risk neutral argument. In 1970, after they attempted to apply the formula to the markets and incurred financial losses due to lack of risk management in their trades, they decided to focus in their domain area, the academic environment. After three years of efforts, the formula named in honor of them for making it public, was finally published in 1973 in an article entitled ""The Pricing of Options and Corporate Liabilities"", in the Journal of Political Economy. Robert C. Merton was the first to publish a paper expanding the mathematical understanding of the options pricing model, and coined the term ""Black–Scholes options pricing model"". Merton and Scholes received the 1997 Nobel Memorial Prize in Economic Sciences for their work, the committee citing their discovery of the risk neutral dynamic revision as a breakthrough that separates the option from the risk of the underlying security. Although ineligible for the prize because of his death in 1995, Black was mentioned as a contributor by the Swedish Academy. The key idea behind the model is to hedge the option by buying and selling the underlying asset in just the right way and, as a consequence, to eliminate risk. This type of hedging is called ""continuously revised delta hedging"" and is the basis of more complicated hedging strategies such as those engaged in by investment banks and hedge funds. The model's assumptions have been relaxed and generalized in many directions, leading to a plethora of models that are currently used in derivative pricing and risk management. It is the insights of the model, as exemplified in the Black–Scholes formula, that are frequently used by market participants, as distinguished from the actual prices. These insights include no-arbitrage bounds and risk-neutral pricing (thanks to continuous revision). Further, the Black–Scholes equation, a partial differential equation that governs the price of the option, enables pricing using numerical methods when an explicit formula is not possible. The Black–Scholes formula has only one parameter that cannot be directly observed in the market: the average future volatility of the underlying asset, though it can be found from the price of other options. Since the option value (whether put or call) is increasing in this parameter, it can be inverted to produce a ""volatility surface"" that is then used to calibrate other models, e.g. for OTC derivatives."
42,Discretization,"The process of transferring continuous functions, models, variables, and equations into discrete, or isolated, counterparts.","Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","In applied mathematics, discretization is the process of transferring continuous functions, models, variables, and equations into discrete counterparts. This process is usually carried out as a first step toward making them suitable for numerical evaluation and implementation on digital computers. Dichotomization is the special case of discretization in which the number of discrete classes is 2, which can approximate a continuous variable as a binary variable (creating a dichotomy for modeling purposes, as in binary classification). Discretization is also related to discrete mathematics, and is an important component of granular computing. In this context, discretization may also refer to modification of variable or category granularity, as when multiple discrete variables are aggregated or multiple discrete categories fused. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand. The terms discretization and quantization often have the same denotation but not always identical connotations. (Specifically, the two terms share a semantic field.) The same is true of discretization error and quantization error. Mathematical methods relating to discretization include the Euler–Maruyama method and the zero-order hold."
43,Gaussian Process,"A stochastic process, such that every finite collection of random variables has a multivariate normal distribution.","Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space. A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the kernel function) to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information—it is a one-dimensional Gaussian distribution (which is the marginal distribution at that point). For some kernel functions, matrix algebra can be used to calculate the predictions using the technique of kriging. When a parameterised kernel is used, optimisation software is typically used to fit a Gaussian process model. The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions. Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times."
44,Brownian Motion,The random motion of particles suspended in a fluid resulting from their collision with the fast-moving molecules in the fluid.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","Brownian motion or pedesis (from Ancient Greek: πήδησις /pέːdεːsis/ ""leaping"") is the random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid. This pattern of motion typically alternates random fluctuations in a particle's position inside a fluid sub-domain with a relocation to another sub-domain. Each relocation is followed by more fluctuations within the new closed volume. This pattern describes a fluid at thermal equilibrium, defined by a given temperature. Within such fluid there exists no preferential direction of flow as in transport phenomena. More specifically the fluid's overall linear and angular momenta remain null over time. It is important also to note that the kinetic energies of the molecular Brownian motions, together with those of molecular rotations and vibrations sum up to the caloric component of a fluid's internal energy. This motion is named after the botanist Robert Brown, who was the most eminent microscopist of his time. In 1827, while looking through a microscope at pollen of the plant Clarkia pulchella immersed in water, the triangular shaped pollen burst at the corners, emitting particles which he noted jiggled around in the water in random fashion. He was not able to determine the mechanisms that caused this motion. Atoms and molecules had long been theorized as the constituents of matter, and Albert Einstein published a paper in 1905 that explained in precise detail how the motion that Brown had observed was a result of the pollen being moved by individual water molecules, making one of his first big contributions to science. This explanation of Brownian motion served as convincing evidence that atoms and molecules exist, and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 ""for his work on the discontinuous structure of matter"". The direction of the force of atomic bombardment is constantly changing, and at different times the particle is hit more on one side than another, leading to the seemingly random nature of the motion. The many-body interactions that yield the Brownian pattern cannot be solved by a model accounting for every involved molecule. In consequence only probabilistic models applied to molecular populations can be employed to describe it. Two such models of the statistical mechanics, due to Einstein and Smoluchowski are presented below. Another, pure probabilistic class of models is the class of the stochastic process models. There exist both simpler and more complicated stochastic processes which in extreme (""taken to the limit"") may describe the Brownian Motion."
45,Euler-Maruyama Method,A method for the approximate numerical solution of a stochastic differential equation.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","In Itô calculus, the Euler–Maruyama method (also called the Euler method) is a method for the approximate numerical solution of a stochastic differential equation (SDE). It is a simple generalization of the Euler method for ordinary differential equations to stochastic differential equations. It is named after Leonhard Euler and Gisiro Maruyama. Unfortunately, the same generalization cannot be done for any arbitrary deterministic method."
46,Parabolic Partial Differential Equation,"A type of partial differential equation describing a wide variety of time-dependent phenomena, including heat conduction, particle diffusion, ocean acoustic propagation, and pricing of derivative investment instruments.","Janusz Mierczyński, Globally positive solutions of linear parabolic partial differential equations of second order with Dirichlet boundary conditions (Cornell University Library, 2017) https://arxiv.org/abs/1708.06813.","For a general space dimension n, the present author in [7] has considered the problem of characterizing globally positive solutions of a linear second order parabolic PDE under Robin (regular oblique) boundary conditions. The reasoning from that paper, however, fails to carry over to general boundary conditions."
47,Differential Equation,A mathematical equation relating some function with its derivatives.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","A differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology. In pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions—the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form. If a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy."
48,Partial Derivative,A partial differential equation is a differential equation containing unknown multivariable functions and their partial derivatives.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","In mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry."
49,Kolmogorov Equations,"Characterize stochastic processes in probability theory, specifically describing how the probability that a stochastic process is in a certain state changes over time.","Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","Writing in 1931, Andrei Kolmogorov started from the theory of discrete time Markov processes, which are described by the Chapman-Kolmogorov equation, and sought to derive a theory of continuous time Markov processes by extending this equation. He found that there are two kinds of continuous time Markov processes, depending on the assumed behavior over small intervals of time: If you assume that ""in a small time interval there is an overwhelming probability that the state will remain unchanged; however, if it changes, the change may be radical"" then you are led to what are called jump processes. The other case leads to processes such as those ""represented by diffusion and by Brownian motion; there it is certain that some change will occur in any time interval, however small; only, here it is certain that the changes during small time intervals will be also small"". For each of these two kinds of processes, Kolmogorov derived a forward and a backward system of equations (four in all)."
50,Monte Carlo Simulation,A computerized mathematical technique allowing people to account for risk in quantitative analysis and decision making.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","Monte Carlo Simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of spreadsheet costs construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo Simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis because the “what if” analysis gives equal weight to all scenarios, while the Monte Carlo method hardly samples in the very low probability regions."
51,Derivative,An expression representing the rate of change of a function with respect to an independent variable.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances. The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the ""instantaneous rate of change"", the ratio of the instantaneous change in the dependent variable to that of the independent variable. Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector. The process of finding a derivative is called differentiation. The reverse process is called antidifferentiation. The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus."
52,First-Order Logic,A form of symbolized reasoning used in computational linguistics.,"Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","First-order logic—also known as first-order predicate calculus and predicate logic—is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form ""there exists X such that X is Socrates and X is a man"" and there exists is a quantifier while X is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations. A theory about a topic is usually a first-order logic together with a specified domain of discourse over which the quantified variables range, finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold for those things. Sometimes ""theory"" is understood in a more formal sense, which is just a set of sentences in first-order logic. The adjective ""first-order"" distinguishes first-order logic from higher-order logic in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets. There are many deductive systems for first-order logic which are both sound (all provable statements are true in all models) and complete (all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the Löwenheim–Skolem theorem and the compactness theorem. First-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics. Peano arithmetic and Zermelo–Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic. No first-order theory, however, has the strength to uniquely describe a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic. The foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see José Ferreirós (2001)."
53,Term,An individual variable or individual constant in First-Order Logic.,"Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","The set of terms is inductively defined by the following rules: Variables. Any variable is a term. Functions. Any expression f(t1,...,tn) of n arguments (where each argument ti is a term and f is a function symbol of valence n) is a term. In particular, symbols denoting individual constants are nullary function symbols, and are thus terms. Only expressions which can be obtained by finitely many applications of rules 1 and 2 are terms. For example, no expression involving a predicate symbol is a term."
54,Argument,"An argument is a series of statements intended to determine the degree of truth of another statement, the conclusion.","Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","In logic and philosophy, an argument is a series of statements (in a natural language), called the premises or premisses (both spellings are acceptable) intended to determine the degree of truth of another statement, the conclusion. The logical form of an argument in a natural language can be represented in a symbolic formal language, and independently of natural language formally defined ""arguments"" can be made in math and computer science. Logic is the study of the forms of reasoning in arguments and the development of standards and criteria to evaluate arguments. Deductive arguments can be valid or sound: in a valid argument, premisses necessitate the conclusion, even if one or more of the premisses is false and the conclusion is false; in a sound argument, true premisses necessitate a true conclusion. Inductive arguments, by contrast, can have different degrees of logical strength: the stronger or more cogent the argument, the greater the probability that the conclusion is true, the weaker the argument, the lesser that probability. The standards for evaluating non-deductive arguments may rest on different or additional criteria than truth—for example, the persuasiveness of so-called ""indispensability claims"" in transcendental arguments, the quality of hypotheses in retroduction, or even the disclosure of new possibilities for thinking and acting."
55,Logical Consequence,Describes the relationship between statements that hold true when one statement logically follows from one or more statements.,"Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","Logical consequence (also entailment) is a fundamental concept in logic, which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises? All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth. Logical consequence is necessary and formal, by way of examples that explain with formal proof and models of interpretation. A sentence is said to be a logical consequence of a set of sentences, for a given language, if and only if, using only logic (i.e. without regard to any personal interpretations of the sentences) the sentence must be true if every sentence in the set is true."
56,Predicate,"In First-Order Logic, a predicate is a Boolean Function of arguments.","Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","A predicate takes an entity or entities in the domain of discourse as input while outputs are either True or False. Consider the two sentences ""Socrates is a philosopher"" and ""Plato is a philosopher"". In propositional logic, these sentences are viewed as being unrelated and might be denoted, for example, by variables such as p and q. The predicate ""is a philosopher"" occurs in both sentences, which have a common structure of ""a is a philosopher"". The variable a is instantiated as ""Socrates"" in the first sentence and is instantiated as ""Plato"" in the second sentence. While first-order logic allows for the use of predicates, such as ""is a philosopher"" in this example, propositional logic does not. Relationships between predicates can be stated using logical connectives. Consider, for example, the first-order formula ""if a is a philosopher, then a is a scholar"". This formula is a conditional statement with ""a is a philosopher"" as its hypothesis and ""a is a scholar"" as its conclusion. The truth of this formula depends on which object is denoted by a, and on the interpretations of the predicates ""is a philosopher"" and ""is a scholar"". Quantifiers can be applied to variables in a formula. The variable a in the previous formula can be universally quantified, for instance, with the first-order sentence ""For every a, if a is a philosopher, then a is a scholar"". The universal quantifier ""for every"" in this sentence expresses the idea that the claim ""if a is a philosopher, then a is a scholar"" holds for all choices of a. The negation of the sentence ""For every a, if a is a philosopher, then a is a scholar"" is logically equivalent to the sentence ""There exists a such that a is a philosopher and a is not a scholar"". The existential quantifier ""there exists"" expresses the idea that the claim ""a is a philosopher and a is not a scholar"" holds for some choice of a. The predicates ""is a philosopher"" and ""is a scholar"" each take a single variable. In general, predicates can take several variables. In the first-order sentence ""Socrates is the teacher of Plato"", the predicate ""is the teacher of"" takes two variables. An interpretation (or model) of a first-order formula specifies what each predicate means and the entities that can instantiate the variables. These entities form the domain of discourse or universe, which is usually required to be a nonempty set. For example, in an interpretation with the domain of discourse consisting of all human beings and the predicate ""is a philosopher"" understood as ""was the author of the Republic"", the sentence ""There exists a such that a is a philosopher"" is seen as being true, as witnessed by Plato."
57,Algorithm,A step-by-step list of instructions for solving a problem.,"Bradley N. Miller, David L. Ranum, Problem Solving with Algorithms and Data Structures Using Python (2nd. Franklin, Beedle & Associates 2011).","In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (About this soundlisten)) is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. The concept of algorithm has existed for centuries. Greek mathematicians used algorithms in, for example, the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbers. The word algorithm itself derives from the 9th century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability"" or ""effective method"" Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939."
58,Byte,A system of logic that accepts only two values: true or false.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","The byte is a unit of digital information that most commonly consists of eight bits, representing a binary number. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures. The size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size – byte-sizes from 1 to 48 bits are known to have been used in the past. Early character encoding systems often used six bits, and machines using six-bit and nine-bit bytes were common into the 1960s. These machines most commonly had memory words of 12, 24, 36, 48 or 60 bits, corresponding to two, four, six, eight or 10 six-bit bytes. In this era, bytes in the instruction stream were often referred to as syllables, before the term byte became common. The modern de-facto standard of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient power of two permitting the values 0 through 255 for one byte (2 in power of 8 = 256, where zero signifies number as well). The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the eight-bit size. Modern architectures typically use 32- or 64-bit words, built of four or eight bytes."
59,Bit,Binary digit – 1 or 0.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","The bit (a portmanteau of binary digit) is a basic unit of information used in computing and digital communications. A binary digit can have only one of two values, and may be physically represented with a two-state device. These state values are most commonly represented as either a 0or1. The two values of a binary digit can also be interpreted as logical values (true/false, yes/no), algebraic signs (+/−), activation states (on/off), or any other two-valued attribute. The correspondence between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. The length of a binary number may be referred to as its bit-length. In information theory, one bit is typically defined as the information entropy of a binary random variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known. Confusion often arises because the words bit and binary digit are used interchangeably. But, within Shannon's information theory, a bit and a binary digit are fundamentally different types of entities. A binary digit is a number that can adopt one of two possible values (0 or 1), whereas a bit is the maximum amount of information that can be conveyed by a binary digit (when averaged over both of its states). By analogy, just as a pint-sized bottle can contain between zero and one pint, so a binary digit can convey between zero and one bit of information. A less confusing terminology is to refer to bits as shannons."
60,Compiler,"A specialized computer program that accepts as input commands written in a form familiar to humans and produces as output instructions that a machine can execute, usually in the form of strings and binary numbers.","Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A compiler is a computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program. However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis. A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness. Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages."
61,Quantum Computing,Computational methods modeling the behavior of quantum mechanical systems.,"Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","Quantum computing is computing using quantum-mechanical phenomena, such as superposition and entanglement. A quantum computer is a device that performs quantum computing. Such a computer is different from binary digital electronic computers based on transistors. Whereas common digital computing requires that the data be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses quantum bits or qubits, which can be in superpositions of states. A quantum Turing machine is a theoretical model of such a computer and is also known as the universal quantum computer. The field of quantum computing was initiated by the work of Paul Benioff and Yuri Manin in 1980, Richard Feynman in 1982, and David Deutsch in 1985. As of 2018, the development of actual quantum computers is still in its infancy, but experiments have been carried out in which quantum computational operations were executed on a very small number of quantum bits. Both practical and theoretical research continues, and many national governments and military agencies are funding quantum computing research in additional effort to develop quantum computers for civilian, business, trade, environmental and national security purposes, such as cryptanalysis. Noisy devices with a small number of qubits have been developed by a number of companies, including IBM, Intel, and Google. IBM has made 5-qubit and 16-qubit quantum computing devices available to the public for experiments via the cloud on the IBM Q Experience. D-Wave Systems has been developing their own version of a quantum computer that uses annealing. Large-scale quantum computers would theoretically be able to solve certain problems much more quickly than any classical computers that use even the best currently known algorithms, like integer factorization using Shor's algorithm (which is a quantum algorithm) and the simulation of quantum many-body systems. There exist quantum algorithms, such as Simon's algorithm, that run faster than any possible probabilistic classical algorithm. A classical computer could in principle (with exponential resources) simulate a quantum algorithm, as quantum computation does not violate the Church–Turing thesis. On the other hand, quantum computers may be able to efficiently solve problems which are not practically feasible on classical computers."
62,General Relativity,Einstein's theory of gravity invoking the curvature of space-time.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","General relativity (GR, also known as the general theory of relativity or GTR) is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. Some predictions of general relativity differ significantly from those of classical physics, especially concerning the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light. Examples of such differences include gravitational time dilation, gravitational lensing, the gravitational redshift of light, and the gravitational time delay. The predictions of general relativity in relation to classical physics have been confirmed in all observations and experiments to date. Although general relativity is not the only relativistic theory of gravity, it is the simplest theory that is consistent with experimental data. However, unanswered questions remain, the most fundamental being how general relativity can be reconciled with the laws of quantum physics to produce a complete and self-consistent theory of quantum gravity. Einstein's theory has important astrophysical implications. For example, it implies the existence of black holes—regions of space in which space and time are distorted in such a way that nothing, not even light, can escape—as an end-state for massive stars. There is ample evidence that the intense radiation emitted by certain kinds of astronomical objects is due to black holes; for example, microquasars and active galactic nuclei result from the presence of stellar black holes and supermassive black holes, respectively. The bending of light by gravity can lead to the phenomenon of gravitational lensing, in which multiple images of the same distant astronomical object are visible in the sky. General relativity also predicts the existence of gravitational waves, which have since been observed directly by the physics collaboration LIGO. In addition, general relativity is the basis of current cosmological models of a consistently expanding universe. Widely acknowledged as a theory of extraordinary beauty, general relativity has often been described as the most beautiful of all existing physical theories."
63,M-Theory,Incomplete theory unifying all five versions of string theory; a fully quantum mechanical theory of all force and matter.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","M-theory is a theory in physics that unifies all consistent versions of superstring theory. The existence of such a theory was first conjectured by Edward Witten at a string theory conference at the University of Southern California in the spring of 1995. Witten's announcement initiated a flurry of research activity known as the second superstring revolution. Prior to Witten's announcement, string theorists had identified five versions of superstring theory. Although these theories appeared, at first, to be very different, work by several physicists showed that the theories were related in intricate and nontrivial ways. In particular, physicists found that apparently distinct theories could be unified by mathematical transformations called S-duality and T-duality. Witten's conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called eleven-dimensional supergravity. Although a complete formulation of M-theory is not known, the theory should describe two- and five-dimensional objects called branes and should be approximated by eleven-dimensional supergravity at low energies. Modern attempts to formulate M-theory are typically based on matrix theory or the AdS/CFT correspondence. According to Witten, M should stand for “magic”, “mystery”, or “membrane” according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known. Investigations of the mathematical structure of M-theory have spawned important theoretical results in physics and mathematics. More speculatively, M-theory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect M-theory to experiment typically focus on compactifying its extra dimensions to construct candidate models of our four-dimensional world, although so far none has been verified to give rise to physics as observed in high energy physics experiments."
64,Inertia,Property of a particle resisting acceleration.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","Inertia is the resistance, of any physical object, to any change in its velocity. This includes changes to the object's speed, or direction of motion. An aspect of this property is the tendency of objects to keep moving in a straight line at a constant speed, when no forces are upon them—and this aspect in particular is also called inertia. The principle of inertia is one of the fundamental principles in classical physics that are still used today to describe the motion of objects and how they are affected by the applied forces on them. Inertia comes from the Latin word, iners, meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his Philosophiæ Naturalis Principia Mathematica, which states: The vis insita, or innate force of matter, is a power of resisting by which every  body, as much as in it lies, endeavours to preserve its present state, whether it be of rest or of moving uniformly forward in a straight line. In common usage, the term ""inertia"" may refer to an object's ""amount of resistance to change in velocity"" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term ""inertia"" is more properly understood as shorthand for ""the principle of inertia"" as described by Newton in his First Law of Motion: an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change. On the surface of the Earth, inertia is often masked by the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest), and gravity."
65,Observable Universe,The part of the universe humans can see.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","The observable universe is a spherical region of the Universe comprising all matter that can be observed from Earth at the present time, because electromagnetic radiation from these objects has had time to reach Earth since the beginning of the cosmological expansion. There are at least 2 trillion galaxies in the observable universe. Assuming the Universe is isotropic, the distance to the edge of the observable universe is roughly the same in every direction. That is, the observable universe has a spherical volume (a ball) centered on the observer. Every location in the Universe has its own observable universe, which may or may not overlap with the one centered on Earth. The word observable in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. Because no signals can travel faster than light, any object farther away from us than light could travel in the age of the Universe (estimated as of 2015 around 13.799±0.021 billion years) simply cannot be detected, as they have not reached us yet. Sometimes astrophysicists distinguish between the visible universe, which includes only signals emitted since recombination—and the observable universe, which includes signals since the beginning of the cosmological expansion (the Big Bang in traditional physical cosmology, the end of the inflationary epoch in modern cosmology). According to calculations, the current comoving distance—proper distance, which takes into account that the universe has expanded since the light was emitted—to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represent the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years), while the comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years),[10] about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years[11][12] and its diameter about 28.5 gigaparsecs (93 billion light-years, 8.8×1023 kilometres or 5.5×1023 miles).[13] The total mass of ordinary matter in the universe can be calculated using the critical density and the diameter of the observable universe to be about 1.5 × 1053 kg. In November 2018, astronomers reported that the extragalactic background light (EBL) amounted to 4 × 1084 photons. Since the expansion of the universe is known to accelerate and will become exponential in the future, the light emitted from all distant objects, past some time dependent on their current redshift, will never reach the Earth. In the future all currently observable objects will slowly freeze in time while emitting progressively redder and fainter light. For instance, objects with the current redshift z from 5 to 10 will remain observable for no more than 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 billion parsecs) will never reach Earth."
66,Photon,Messenger particle of the electromagnetic force.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","The photon is a type of elementary particle, the quantum of the electromagnetic field including electromagnetic radiation such as light, and the force carrier for the electromagnetic force (even when static via virtual particles). The photon has zero rest mass and always moves at the speed of light within a vacuum. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of both waves and particles. For example, a single photon may be refracted by a lens and exhibit wave interference with itself, and it can behave as a particle with definite and finite measurable position or momentum, though not both at the same time as per the Heisenberg's uncertainty principle. The photon's wave and quantum qualities are two observable aspects of a single phenomenon – they cannot be described by any mechanical model; a representation of this dual property of light that assumes certain points on the wavefront to be the seat of the energy is not possible. The quanta in a light wave are not spatially localized. The modern concept of the photon was developed gradually by Albert Einstein in the early 20th century to explain experimental observations that did not fit the classical wave model of light. The benefit of the photon model is that it accounts for the frequency dependence of light's energy, and explains the ability of matter and electromagnetic radiation to be in thermal equilibrium. The photon model accounts for anomalous observations, including the properties of black-body radiation, that others (notably Max Planck) had tried to explain using semiclassical models. In that model, light is described by Maxwell's equations, but material objects emit and absorb light in quantized amounts (i.e., they change energy only by certain particular discrete amounts). Although these semiclassical models contributed to the development of quantum mechanics, many further experiments beginning with the phenomenon of Compton scattering of single photons by electrons, validated Einstein's hypothesis that light itself is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name ""photon"" for these particles. After Arthur H. Compton won the Nobel Prize in 1927 for his scattering studies, most scientists accepted that light quanta have an independent existence, and the term ""photon"" was accepted. In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass, and spin, are determined by this gauge symmetry. The photon concept has led to momentous advances in experimental and theoretical physics, including lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography."
67,Quark,Elementary particle subject to the strong nuclear force.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","A quark (/kwɔːrk, kwɑːrk/) is a type of elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei. Due to a phenomenon known as color confinement, quarks are never directly observed or found in isolation; they can be found only within hadrons, which include baryons (such as protons and neutrons) and mesons. For this reason, much of what is known about quarks has been drawn from observations of hadrons. Quarks have various intrinsic properties, including electric charge, mass, color charge, and spin. They are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as fundamental forces (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge. There are six types, known as flavors, of quarks: up, down, strange, charm, bottom, and top. Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an antiquark, that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign. The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964. Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968. Accelerator experiments have provided evidence for all six flavors. The top quark, first observed at Fermilab in 1995, was the last to be discovered."
68,Superposition,The property allowing a single quantum particle to exist in two distinct points in space-time simultaneously.,"Eleanor Rieffel, Wolfgang Polak, Quantum Computing (The MIT Press 2014).","Quantum superposition is a fundamental principle of quantum mechanics. It states that, much like waves in classical physics, any two (or more) quantum states can be added together (""superposed"") and the result will be another valid quantum state; and conversely, that every quantum state can be represented as a sum of two or more other distinct states. Mathematically, it refers to a property of solutions to the Schrödinger equation; since the Schrödinger equation is linear, any linear combination of solutions will also be a solution. An example of a physically observable manifestation of the wave nature of quantum systems is the interference peaks from an electron beam in a double-slit experiment. The pattern is very similar to the one obtained by diffraction of classical waves. Another example is a quantum logical qubit state, as used in quantum information processing, which is a quantum superposition of the ""basis states"" Contrary to a classical bit that can only be in the state corresponding to 0 or the state corresponding to 1, a qubit may be in a superposition of both states. This means that the probabilities of measuring 0 or 1 for a qubit are in general neither 0.0 nor 1.0, and multiple measurements made on qubits in identical states will not always give the same result."
69,Uncertainty Principle,It is impossible to precisely know the position and velocity of a particle.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","In quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position x and momentum p, can be known. Introduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. Historically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical ""explanation"" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology. It must be emphasized that measurement does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer. Since the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers."
70,Time-Symmetry,"Property of quantum mechanics referring to the symmetrical nature of time, meaning time does not necessarily flow.","Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","Time translation symmetry or temporal translation symmetry (TTS) is a mathematical transformation in physics that moves the times of events through a common interval. Time translation symmetry is the hypothesis that the laws of physics are unchanged, (i.e. invariant) under such a transformation. Time translation symmetry is a rigorous way to formulate the idea that the laws of physics are the same throughout history. Time translation symmetry is closely connected via the Noether theorem, to conservation of energy. In mathematics, the set of all time translations on a given system form a Lie group. There are many symmetries in nature besides time translation, such as spatial translation or rotational symmetries. These symmetries can be broken and explain diverse phenomena such as crystals, superconductivity, and the Higgs mechanism. However, it was thought until very recently that time translation symmetry could not be broken. Time crystals, a state of matter first observed in 2017, break time translation symmetry."
71,Entanglement,Phenomenon where spatially distinct particles have correlated or dependent properties.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","Quantum entanglement is a quantum mechanical phenomenon in which the quantum states of two or more objects have to be described with reference to each other, even though the individual objects may be spatially separated. This leads to correlations between observable physical properties of the systems. For example, it is possible to prepare two particles in a single quantum state such that when one is observed to be spin-up, the other one will always be observed to be spin-down and vice versa, this despite the fact that it is impossible to predict, according to quantum mechanics, which set of measurements will be observed. As a result, measurements performed on one system seem to be instantaneously influencing other systems entangled with it. But quantum entanglement does not enable the transmission of classical information faster than the speed of light. Quantum entanglement has applications in the emerging technologies of quantum computing and quantum cryptography, and has been used to realize quantum teleportation experimentally. At the same time, it prompts some of the more philosophically oriented discussions concerning quantum theory. The correlations predicted by quantum mechanics, and observed in experiment, reject the principle of local realism, which is that information about the state of a system should only be mediated by interactions in its immediate surroundings. Different views of what is actually occurring in the process of quantum entanglement can be related to different interpretations of quantum mechanics."
72,Non-Locality,"Principle in quantum mechanics referring to the observation that two objects can be far apart in space-time, but act in quantum systems as a single entity.","Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In theoretical physics, quantum nonlocality most commonly refers to the phenomenon by which measurements made at a microscopic level contradict a collection of notions known as local realism that are regarded as intuitively true in classical mechanics. However, some quantum mechanical predictions of multi-system measurement statistics on entangled quantum states cannot be simulated by any local hidden variable theory. An explicit example is demonstrated by Bell's theorem, which has been verified by experiment. Experiments have generally favored quantum mechanics as a description of nature, over local hidden variable theories. Any physical theory that supersedes or replaces quantum theory must make similar experimental predictions and must therefore also be nonlocal in this sense; quantum nonlocality is a property of the universe that is independent of our description of nature. Quantum nonlocality does not allow for faster-than-light communication, and hence is compatible with special relativity. However, it prompts many of the foundational discussions concerning quantum theory. In October 2018, physicists reported that quantum behavior can be explained with classical physics for a single particle, but not for multiple particles as in quantum entanglement and related nonlocality phenomena."
73,Quantum Tunneling,Property of quantum mechanics where a particle passes through an object it could not pass through under classical laws.,"D-Wave System Documentation, Getting Started with the D-Wave System, https://docs.dwavesys.com/docs/latest/index.html (2018).","Quantum tunnelling or tunneling (see spelling differences) is the quantum mechanical phenomenon where a particle passes through a potential barrier that it classically cannot surmount. This plays an essential role in several physical phenomena, such as the nuclear fusion that occurs in main sequence stars like the Sun. It has important applications to modern devices such as the tunnel diode, quantum computing, and the scanning tunnelling microscope. The effect was predicted in the early 20th century, and its acceptance as a general physical phenomenon came mid-century. Fundamental quantum mechanical concepts are central to this phenomenon, which makes quantum tunnelling one of the novel implications of quantum mechanics. Quantum tunneling is projected to create physical limits to the size of transistors, due to electrons being able to tunnel past them if the transistors are too small. Tunnelling is often explained in terms of the Heisenberg uncertainty principle and the premise that the quantum object has more than one fixed state (not a wave nor a particle) in general."
74,Computer network,A digital telecommunications architecture allowing nodes to share resources.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A computer network, or data network, is a digital telecommunications network which allows nodes to share resources. In computer networks, computing devices exchange data with each other using connections (data links) between nodes. These data links are established over cable media such as wires or optic cables, or wireless media such as WiFi. Network computer devices that originate, route and terminate the data are called network nodes. Nodes are identified by network addresses and can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols. This formidable collection of information technology requires skilled network management to keep it all running reliably. Computer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. Computer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology, traffic control mechanism and organizational intent. The best-known computer network is the Internet."
75,Public Cloud,Provides services to anyone having Internet access.,"Nayan B. Ruparelia, Cloud Computing, 15 (MIT Press, 2016).","A cloud is called a ""public cloud"" when the services are rendered over a network that is open for public use. Public cloud services may be free. Technically there may be little or no difference between public and private cloud architecture, however, security consideration may be substantially different for services (applications, storage, and other resources) that are made available by a service provider for a public audience and when communication is effected over a non-trusted network. Generally, public cloud service providers like Amazon Web Services (AWS), Oracle, Microsoft and Google own and operate the infrastructure at their data center and access is generally via the Internet. AWS, Oracle, Microsoft, and Google also offer direct connect services called ""AWS Direct Connect"", ""Oracle FastConnect"", ""Azure ExpressRoute"", and ""Cloud Interconnect"" respectively, such connections require customers to purchase or lease a private connection to a peering point offered by the cloud provider."
76,Private Cloud,"Provides services to a single entity, either a government organization or a business enterprise, such that cloud services are provided to that entity from its own private network.","Nayan B. Ruparelia, Cloud Computing, 15 (MIT Press, 2016).","Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally. Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users ""still have to buy, build, and manage them"" and thus do not benefit from less hands-on management, essentially ""[lacking] the economic model that makes cloud computing such an intriguing concept""."
77,Community Cloud,Provides a middle ground between a public and private cloud.,"Nayan B. Ruparelia, Cloud Computing, 15 (MIT Press, 2016).","A community cloud in computing is a collaborative effort in which infrastructure is shared between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party and hosted internally or externally. This is controlled and used by a group of organizations that have shared interest. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized."
78,Broad Network Access,Refers to the ability to consume resources from any location.,"Nayan B. Ruparelia, Cloud Computing, 17 (MIT Press, 2016).","Definition - What does Broad Network Access mean? Broad network access refers to resources hosted in a private cloud network (operated within a company's firewall) that are available for access from a wide range of devices, such as tablets, PCs, Macs and smartphones. These resources are also accessible from a wide range of locations that offer online access. Companies that have broad network access within a cloud network need to deal with certain security issues that arise. It's a debated topic because it touches at the heart of the difference between private and public cloud computing. Oftentimes, companies choose private cloud service because they are concerned about the potential for information leaks through the gaps left open to outside networks in a public cloud."
79,On-demand Self-service,Refers to the ability to access services when needed.,"Nayan B. Ruparelia, Cloud Computing, 17 (MIT Press, 2016).","Cloud computing provides resources on demand, i.e. when the consumer wants it. This is made possible by self-service and automation. Self-service means that the consumer performs all the actions needed to acquire the service herself, instead of going through an IT department, for example. The consumer’s request is then automatically processed by the cloud infrastructure, without human intervention on the provider’s side. To make this possible, a cloud provider must obviously have the infrastructure in place to automatically handle consumers’ requests. Most likely, this infrastructure will be virtualized, so different consumers can use the same pooled hardware. On-demand self-service computing implies a high level of planning. For instance, a cloud consumer can request a new virtual machine at any time, and expects to have it working in a couple of minutes. The underlying hardware, however, might take 90 days to get delivered to the provider. It is therefore necessary to monitor trends in resource usage and plan for future situations well in advance."
80,Random Access Memmory,The portion of a computer’s memory with the highest access speeds.,"Paul E. Ceruzzi, Computing: A Concise History, 173 (MIT Press, 2012).","Random-access memory (RAM /ræm/) is a form of computer data storage that stores data and machine code currently being used. A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory. In contrast, with other direct-access data storage media such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory, the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement. RAM contains multiplexing and demultiplexing circuitry, to connect the data lines to the addressed storage for reading or writing the entry. Usually more than one bit of storage is accessed by the same address, and RAM devices often have multiple data lines and are said to be ""8-bit"" or ""16-bit"", etc. devices. In today's technology, random-access memory takes the form of integrated circuits. RAM is normally associated with volatile types of memory (such as DRAM modules), where stored information is lost if power is removed, although non-volatile RAM has also been developed. Other types of non-volatile memories exist that allow random access for read operations, but either do not allow write operations or have other kinds of limitations on them. These include most types of ROM and a type of flash memory called NOR-Flash. Integrated-circuit RAM chips came into the market in the early 1970s, with the first commercially available DRAM chip, the Intel 1103, introduced in October 1970."
81,Firewall,First line of defense to deter security breach. The firewall contains rules allowing it make decisions about which traffic to allow through and which traffic to block.,"Nayan B. Ruparelia, Cloud Computing, 106 (MIT Press, 2016).","In computing, a firewall is a network security system that monitors and controls incoming and outgoing network traffic based on predetermined security rules. A firewall typically establishes a barrier between a trusted internal network and untrusted external network, such as the Internet. Firewalls are often categorized as either network firewalls or host-based firewalls. Network firewalls filter traffic between two or more networks and run on network hardware. Host-based firewalls run on host computers and control network traffic in and out of those machines."
82,Server,"A server is a computer program or device providing functionality for other programs or devices, called clients. Typically, a server contains a large memory, delivering data over networks at high speeds.","Paul E. Ceruzzi, Computing: A Concise History, 174 (MIT Press, 2012).","In computing, a server is a computer program or a device that provides functionality for other programs or devices, called ""clients"". This architecture is called the client–server model, and a single overall computation is distributed across multiple processes or devices. Servers can provide various functionalities, often called ""services"", such as sharing data or resources among multiple clients, or performing computation for a client. A single server can serve multiple clients, and a single client can use multiple servers. A client process may run on the same device or may connect over a network to a server on a different device. Typical servers are database servers, file servers, mail servers, print servers, web servers, game servers, and application servers. Client–server systems are today most frequently implemented by (and often identified with) the request–response model: a client sends a request to the server, which performs some action and sends a response back to the client, typically with a result or acknowledgement. Designating a computer as ""server-class hardware"" implies that it is specialized for running servers on it. This often implies that it is more powerful and reliable than standard personal computers, but alternatively, large computing clusters may be composed of many relatively simple, replaceable server components."
83,Client,A piece of computer hardware or software that accesses a service made available by a server.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A client is a computer or a program that, as part of its operation, relies on sending a request to another program or a computer hardware or software that accesses a service made available by a server(which may or may not be located on another computer). For example, web browsers are clients that connect to web servers and retrieve web pages for display. Email clients retrieve email from mail servers. Online chat uses a variety of clients, which vary depending on the chat protocol being used. Multiplayer video games or online video games may run as a client on each computer. The term ""client"" may also be applied to computers or devices that run the client software or users that use the client software. A client is part of a client–server model, which is still used today. Clients and servers may be computer programs run on the same machine and connect via inter-process communication techniques. Combined with Internet sockets, programs may connect to a service operating on a possibly remote system through the Internet protocol suite. Servers wait for potential clients to initiate connections that they may accept. The term was first applied to devices that were not capable of running their own stand-alone programs, but could interact with remote computers via a network. These computer terminals were clients of the time-sharing mainframe computer."
84,Client-Server Model,"An arrangement of networked computers, in which clients receive data from servers – large capacity computers. The data are typically in raw form, while the servers use their computing capacity to handle housekeeping, graphics, and other computations.","Paul E. Ceruzzi, Computing: A Concise History, 170 (MIT Press, 2012).","Client–server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host runs one or more server programs which share their resources with clients. A client does not share any of its resources, but requests a server's content or service function. Clients therefore initiate communication sessions with servers which await incoming requests. Examples of computer applications that use the client–server model are Email, network printing, and the World Wide Web."
85,Resource Pooling,"Refers to the ability to pool the infrastructure, virtual platforms, and applications.","Nayan B. Ruparelia, Cloud Computing, 17 (MIT Press, 2016).","Pooling IT (equipment and staff) resources involves virtualization of typical IT stacks server, storage and networking (but also on the level of datacenter power and cooling). Users benefit from lower individual investments since resources are shared. Although shared infrastructures have huge benefits potential issues on the environment have impact on the complete environment. A thorough analysis of the infrastructure is recommended to identify potential single point of failure (SPOF). One may opt for 'private' instances (private clouds) for specific needs or for specific reasons. On the level of resource pooling, bigger suppliers tend to have the benefit of being able to provide shared support environments with round the clock service. Do you prefer access to a service desk round the clock with potential less expertise or do you want to rely on a single support engineer (who is 'on duty' during off peak hours)"
86,Measured Service,Refers to a payment model where the consumer pays for the services as they are consumed.,"Nayan B. Ruparelia, Cloud Computing, 17 (MIT Press, 2016).","Measured service is a term that IT professionals apply to cloud computing. This is a reference to services where the cloud provider measures or monitors the provision of services for various reasons, including billing, effective use of resources, or overall predictive planning."
87,Server Virtualization,Uses common physical hardware to host virtual machines allowing the consolidation of a larger number machines into a smaller number of machines hosting virtual machines.,"Nayan B. Ruparelia, Cloud Computing, 6 (MIT Press, 2016).","Server virtualization is a virtualization technique that involves partitioning a physical server into a number of small, virtual servers with the help of virtualization software. In server virtualization, each virtual server runs multiple operating system instances at the same time."
88,Application Virtualization,Delivers an application that is hosted on a single machine to a large number of users.,"Nayan B. Ruparelia, Cloud Computing, 5 (MIT Press, 2016).","Application Virtualization (also known as Process Virtualization) is software technology that encapsulates computer programs from the underlying operating system on which it is executed. A fully virtualized application is not installed in the traditional sense, although it is still executed as if it were. The application behaves at runtime like it is directly interfacing with the original operating system and all the resources managed by it, but can be isolated or sandboxed to varying degrees. In this context, the term ""virtualization"" refers to the artifact being encapsulated (application), which is quite different from its meaning in hardware virtualization, where it refers to the artifact being abstracted (physical hardware)."
89,Cloud Bursting,A could service or application that instantaneously jumps to use the resources of another cloud to meet an inordinate increase in demand or due to business continuity reasons.,"Nayan B. Ruparelia, Cloud Computing, 247 (MIT Press, 2016).","In cloud computing, cloud bursting is a configuration that’s set up between a private cloud and a public cloud to deal with peaks in IT demand. If an organization using a private cloud reaches 100 percent of its resource capacity, the overflow traffic is directed to a public cloud so there’s no interruption of services. In addition to flexibility and self-service functionality, the key advantage to cloud bursting is economical savings. You only pay for the additional resources when there is a demand for those resources - no more spending on extra capacity you’re not using or trying to predict demand peaks and fluctuations. An application can be applied to the private cloud, then burst to the public cloud only when necessary to meet peak demands. Plus, cloud bursting can also be used to shoulder processing burdens by moving basic applications to the public cloud to free up local resources for business-critical applications. When using cloud bursting, you should consider security and compliance requirements, latency, load balancing, and platform compatibility."
90,Interoperability,"Capability to use the same or similar cloud services offered by different service providers. Relies on billing, management, reporting, data, and application or process functions to be in place.","Nayan B. Ruparelia, Cloud Computing, 248 (MIT Press, 2016).","Interoperability is a characteristic of a product or system, whose interfaces are completely understood, to work with other products or systems, at present or in the future, in either implementation or access, without any restrictions. While the term was initially defined for information technology or systems engineering services to allow for information exchange, a broader definition takes into account social, political, and organizational factors that impact system to system performance. Task of building coherent services for users when the individual components are technically different and managed by different organizations."
91,Virtualization,"There are two basic types of virtualization, server virtualization and application virtualization.","Nayan B. Ruparelia, Cloud Computing, 5 (MIT Press, 2016).","In computing, virtualization refers to the act of creating a virtual (rather than actual) version of something, including virtual computer hardware platforms, storage devices, and computer network resources. Virtualization began in the 1960s, as a method of logically dividing the system resources provided by mainframe computers between different applications. Since then, the meaning of the term has broadened."
92,Binary Quadratic Model,A collection of binary-valued variables with associated linear and quadratic biases.,"D-Wave System Documentation, Glossary (2018) https://docs.ocean.dwavesys.com/en/latest/glossary.html#term-binary-quadratic-model.","When the coefficients can be arbitrary complex numbers, most results are not specific to the case of two variables, so they are described in quadratic form. A quadratic form with integer coefficients is called an integral binary quadratic form, often abbreviated to binary quadratic form. This article is entirely devoted to integral binary quadratic forms. This choice is motivated by their status as the driving force behind the development of algebraic number theory. Since the late nineteenth century, binary quadratic forms have given up their preeminence in algebraic number theory to quadratic and more general number fields, but advances specific to binary quadratic forms still occur on occasion."
93,Security Container,A protected environment of information – where virtual networks are created on a physical network – preventing outsiders from accessing information.,"Nayan B. Ruparelia, Cloud Computing, 106 (MIT Press, 2016).","Docker is a computer program that performs operating-system-level virtualization, also known as ""containerization"". It was first released in 2013 and is developed by Docker, Inc. Docker is used to run software packages called ""containers"". Containers are isolated from each other and bundle their own application, tools, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines. Containers are created from ""images"" that specify their precise contents. Images are often created by combining and modifying standard images downloaded from public repositories."
94,Identification,Ascertains the user’s identity.,"n B. Ruparelia, Cloud Computing (MIT Press, 2016).","Here are some examples of the identification and authentication in an environment: Every message can contain message context information. This information is held in the message descriptor. It can be generated by the queue manager when a message is put on a queue by an application. Alternatively, the application can supply the information if the user ID associated with the application is authorized to do so. The context information in a message allows the receiving application to find out about the originator of the message. It contains, for example, the name of the application that put the message and the user ID associated with the application. When a message channel starts, it is possible for the message channel agent (MCA) at each end of the channel to authenticate its partner. This technique is known as mutual authentication. For the sending MCA, it provides assurance that the partner it is about to send messages to is genuine. For the receiving MCA, there is a similar assurance that it is about to receive messages from a genuine partner."
95,Authentication,Establishes whether the user is a legitimate user.,"n B. Ruparelia, Cloud Computing (MIT Press, 2016).","The process of identifying an individual, usually based on a username and password. In security systems, authentication is distinct from authorization, which is the process of giving individuals access to system objects based on their identity. Authentication merely ensures that the individual is who he or she claims to be, but says nothing about the access rights of the individual."
96,Authorization,Establishes the tasks that an authenicated user is allowed to perform.,"Nayan B. Ruparelia, Cloud Computing, 106 (MIT Press, 2016).","Authorization is the function of specifying access rights/privileges to resources related to information security and computer security in general and to access control in particular. More formally, ""to authorize"" is to define an access policy. For example, human resources staff are normally authorized to access employee records and this policy is usually formalized as access control rules in a computer system. During operation, the system uses the access control rules to decide whether access requests from (authentication) shall be approved (granted) or disapproved (rejected). Resources include individual files or an item's data, computer programs, computer devices and functionality provided by computer applications. Examples of consumers are computer users, computer Software and other Hardware on the computer."
97,Security Procedures,"Steps to secure network upon request for admission. Generally three security procedures are used to enter a container – identification, authentication, and authorization.","Nayan B. Ruparelia, Cloud Computing, 106 (MIT Press, 2016).","Computer security, cybersecurity or information technology security (IT security) is the protection of computer systems from theft or damage to their hardware, software or electronic data, as well as from disruption or misdirection of the services they provide. The field is growing in importance due to increasing reliance on computer systems, the Internet and wireless networks such as Bluetooth and Wi-Fi, and due to the growth of ""smart"" devices, including smartphones, televisions and the various tiny devices that constitute the Internet of things. Due to its complexity, both in terms of politics and technology, it is also one of the major challenges of the contemporary world."
98,Big Bang,The singularity at the begining of the universe.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","The Big Bang theory is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high-density and high-temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law (the farther away galaxies are, the faster they are moving away from Earth). If the observed conditions are extrapolated backwards in time using the known laws of physics, the prediction is that just before a period of very high density there was a singularity which is typically associated with the Big Bang. Physicists are undecided whether this means the universe began from a singularity, or that current knowledge is insufficient to describe the universe at that time. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements (mostly hydrogen, with some helium and lithium) later coalesced through gravity, eventually forming early stars and galaxies, the descendants of which are visible today. Astronomers also observe the gravitational effects of dark matter surrounding galaxies. Though most of the mass in the universe seems to be in the form of dark matter, Big Bang theory and various observations seem to indicate that it is not made out of conventional baryonic matter (protons, neutrons, and electrons) but it is unclear exactly what it is made out of. Since Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. The scientific community was once divided between supporters of two different theories, the Big Bang and the Steady State theory, but a wide range of empirical evidence has strongly favored the Big Bang which is now universally accepted. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature."
99,Black Hole,"A region of space-time from which nothing, not even light, can escape, because gravity is so strong.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A black hole is a region of spacetime exhibiting such strong gravitational effects that nothing—not even particles and electromagnetic radiation such as light—can escape from inside it. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of the region from which no escape is possible is called the event horizon. Although the event horizon has an enormous effect on the fate and circumstances of an object crossing it, no locally detectable features appear to be observed. In many ways a black hole acts like an ideal black body, as it reflects no light. Moreover, quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is on the order of billionths of a kelvin for black holes of stellar mass, making it essentially impossible to observe. Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. The first modern solution of general relativity that would characterize a black hole was found by Karl Schwarzschild in 1916, although its interpretation as a region of space from which nothing can escape was first published by David Finkelstein in 1958. Black holes were long considered a mathematical curiosity; it was during the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery of neutron stars in the late 1960s sparked interest in gravitationally collapsed compact objects as a possible astrophysical reality. Black holes of stellar mass are expected to form when very massive stars collapse at the end of their life cycle. After a black hole has formed, it can continue to grow by absorbing mass from its surroundings. By absorbing other stars and merging with other black holes, supermassive black holes of millions of solar masses (M☉) may form. There is general consensus that supermassive black holes exist in the centers of most galaxies. Despite its invisible interior, the presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter that falls onto a black hole can form an external accretion disk heated by friction, forming some of the brightest objects in the universe. If there are other stars orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems, and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses. On 11 February 2016, the LIGO collaboration announced the first direct detection of gravitational waves, which also represented the first observation of a black hole merger. As of December 2018, eleven gravitational wave events have been observed that originated from ten merging black holes (along with one binary neutron star merger)."
100,Cosmology,The study of the universe as a whole,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Cosmology (from the Greek κόσμος, kosmos ""world"" and -λογία, -logia ""study of"") is the study of the origin, evolution, and eventual fate of the universe. Physical cosmology is the scientific study of the universe's origin, its large-scale structures and dynamics, and its ultimate fate, as well as the scientific laws that govern these areas. The term cosmology was first used in English in 1656 in Thomas Blount's Glossographia, and in 1731 taken up in Latin by German philosopher Christian Wolff, in Cosmologia Generalis. Religious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation myths and eschatology. Physical cosmology is studied by scientists, such as astronomers and physicists, as well as philosophers, such as metaphysicians, philosophers of physics, and philosophers of space and time. Because of this shared scope with philosophy, theories in physical cosmology may include both scientific and non-scientific propositions, and may depend upon assumptions that cannot be tested. Cosmology differs from astronomy in that the former is concerned with the Universe as a whole while the latter deals with individual celestial objects. Modern physical cosmology is dominated by the Big Bang theory, which attempts to bring together observational astronomy and particle physics; more specifically, a standard parameterization of the Big Bang with dark matter and dark energy, known as the Lambda-CDM model. Theoretical astrophysicist David N. Spergel has described cosmology as a ""historical science"" because ""when we look out in space, we look back in time"" due to the finite nature of the speed of light."
101,Dark Matter,"Matter that can not be observed directly, but can be detected by its gravitational effect.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Dark matter is a hypothetical form of matter that is thought to account for approximately 85% of the matter in the universe, and about a quarter of its total energy density. The majority of dark matter is thought to be non-baryonic in nature, possibly being composed of some as-yet undiscovered subatomic particles. Its presence is implied in a variety of astrophysical observations, including gravitational effects that cannot be explained unless more matter is present than can be seen. For this reason, most experts think dark matter to be ubiquitous in the universe and to have had a strong influence on its structure and evolution. The name dark matter refers to the fact that it does not appear to interact with observable electromagnetic radiation, such as light, and is thus invisible (or 'dark') to the entire electromagnetic spectrum, making it extremely difficult to detect using usual astronomical equipment. The primary evidence for dark matter is that calculations show that many galaxies would fly apart instead of rotating, or would not have formed or move as they do, if they did not contain a large amount of unseen matter. Other lines of evidence include observations in gravitational lensing, from the cosmic microwave background, from astronomical observations of the observable universe's current structure, from the formation and evolution of galaxies, from mass location during galactic collisions, and from the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy of the universe contains 5% ordinary matter and energy, 27% dark matter and 68% of an unknown form of energy known as dark energy. Thus, dark matter constitutes 85%[note 2] of total mass, while dark energy plus dark matter constitute 95% of total mass–energy content. Because dark matter has not yet been observed directly, it must barely interact with ordinary baryonic matter and radiation. The primary candidate for dark matter is some new kind of elementary particle that has not yet been discovered, in particular, weakly-interacting massive particles (WIMPs), or gravitationally-interacting massive particles (GIMPs). Many experiments to directly detect and study dark matter particles are being actively undertaken, but none has yet succeeded. Dark matter is classified as cold, warm, or hot according to its velocity (more precisely, its free streaming length). Current models favor a cold dark matter scenario, in which structures emerge by gradual accumulation of particles. Although the existence of dark matter is generally accepted by the scientific community, some astrophysicists, intrigued by certain observations that do not fit the dark matter theory, argue for various modifications of the standard laws of general relativity, such as modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity. These models attempt to account for all observations without invoking supplemental non-baryonic matter."
102,Light Cone,A surface in space-time that marks out the possible detections for light rays passing through a given event.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","If one imagines the light confined to a two-dimensional plane, the light from the flash spreads out in a circle after the event E occurs, and if we graph the growing circle with the vertical axis of the graph representing time, the result is a cone, known as the future light cone. The past light cone behaves like the future light cone in reverse, a circle which contracts in radius at the speed of light until it converges to a point at the exact position and time of the event E. In reality, there are three space dimensions, so the light would actually form an expanding or contracting sphere in three-dimensional (3D) space rather than a circle in 2D, and the light cone would actually be a four-dimensional version of a cone whose cross-sections form 3D spheres (analogous to a normal three-dimensional cone whose cross-sections form 2D circles), but the concept is easier to visualize with the number of spatial dimensions reduced from three to two. This view of special relativity was first proposed by Albert Einstein's former professor Hermann Minkowski and is known as Minkowski space. The purpose was to create an invariant spacetime for all observers. To uphold causality, Minkowski restricted spacetime to non-Euclidean hyperbolic geometry. Because signals and other causal influences cannot travel faster than light (see special relativity), the light cone plays an essential role in defining the concept of causality: for a given event E, the set of events that lie on or inside the past light cone of E would also be the set of all events that could send a signal that would have time to reach E and influence it in some way. For example, at a time ten years before E, if we consider the set of all events in the past light cone of E which occur at that time, the result would be a sphere (2D: disk) with a radius of ten light-years centered on the position where E will occur. So, any point on or inside the sphere could send a signal moving at the speed of light or slower that would have time to influence the event E, while points outside the sphere at that moment would not be able to have any causal influence on E. Likewise, the set of events that lie on or inside the future light cone of E would also be the set of events that could receive a signal sent out from the position and time of E, so the future light cone contains all the events that could potentially be causally influenced by E. Events which lie neither in the past or future light cone of E cannot influence or be influenced by E in relativity."
103,Light-second,The distance traveled by light in one second.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A light-second is a unit of length equal to the distance light in empty space travels in one second, and is exactly 299,792,458 meters. The light-second is used in telecommunications, astronomy, and physics. In telecommunications, it measures how much time it must take to send a signal a certain distance."
104,Magnetic Field,"The field responsible for magnetic forces, now incorporated with the electric field as the electromagnetic field.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","The magnetic field is the area around a magnet in which there is magnetic force. Moving electric charges can make magnetic fields. Magnetic fields can usually be seen by magnetic flux lines. At all times the direction of the magnetic field is shown by the direction of the magnetic flux lines. The strength of a magnet has to do with the spaces between the magnetic flux lines. The closer the flux lines are to each other, the stronger the magnet is. The farther away they are, the weaker. The flux lines can be seen by placing iron filings over a magnet. The iron filings move and arrange into the lines. Magnetic fields give power to other particles that are touching the magnetic field. In physics, the magnetic field is a field that passes through space and which makes a magnetic force move electric charges and magnetic dipoles. Magnetic fields are around electric currents, magnetic dipoles, and changing electric fields. When placed in a magnetic field, magnetic dipoles are in one line with their axes to be parallel with the field lines, as can be seen when iron filings are in the presence of a magnet. Magnetic fields also have their own energy and momentum, with an energy density proportional to the square of the field intensity. The magnetic field is measured in the units of teslas (SI units) or gauss (cgs units). There are some notable specific kinds of the magnetic field. For the physics of magnetic materials, see magnetism and magnet, and more specifically diamagnetism. For magnetic fields made by changing electric fields, see electromagnetism. The electric field and the magnetic field are components of the electromagnetic field. The law of electromagnetism was founded by Michael Faraday."
105,Neutrino,An extremely light particle that is affected by the weak force and gravity.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Neutrinos are a type of elementary particle that exist all across the universe. Physicists study these particles, but they are hard to find because they have a very small chance of interacting with regular matter. (For example, they pass through the whole earth without touching any other particles). Neutrinos travel near the speed of light. We used to think that they have no mass, but a few years ago physicists found that they have a very small mass, much lighter than electrons. By finding neutrinos, we can learn about the structure and the history of the universe. Neutrinos are very difficult to detect. They are very unlikely to collide (interact, in this case) with other particles as they travel through space or through matter. Unlike negatively charged electrons, neutrinos have no charge (neutrino meaning small neutral particle). This means that they are unaffected by the electromagnetic force, too. Detectors built to find them only see 10-15 a year. They are commonly generated in particle accelerators, the sun, nuclear reactions, and other stars. They are generated whenever there is a nuclear reaction in the form of Beta decay. This process starts off with one neutron, and ends with one electron, one proton, and one neutrino."
106,Neutron,"An uncharged particle, which accounts for nearly half the particles in the nucleus.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Neutrons, with protons and electrons, make up an atom. Neutrons and protons are found in the nucleus of an atom. Unlike protons, which have a positive charge, or electrons, which have a negative charge, neutrons have zero charge which means they are neutral particles. Neutrons bind with protons with the residual strong force. Neutrons were predicted by Ernest Rutherford, and discovered by James Chadwick, in 1932.Atoms were fired at a thin pane of beryllium. Particles emerged which had no charge, and he called these 'neutrons'. They were later added to the modern image of the atom. Neutrons have a mass of 1.675 × 10-24g, which is a little heavier than the proton. Neutrons are 1839 times heavier than electrons. Like all hadrons, neutrons are made of quarks. A neutron is made of two down quarks and one up quark. One up quark has a charge of +2/3, and the two down quarks each have a charge of -1/3. The fact that these charges cancel out is why neutrons have a neutral (0) charge. Quarks are held together by gluons."
107,Proton,"A positively charged particle, that accounts for nearly half the particles in the nucleus of atoms.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A proton is part of an atom. They are found in the nucleus of an atom along with neutrons. The periodic table groups atoms according to how many protons they have. A single atom of hydrogen (the lightest kind of atom) is made up of an electron moving around a proton. Most of the mass of this atom is in the proton, which is almost 2000 times heavier than the electron. Protons and neutrons make up the nucleus of every other kind of atom. In any one element, the number of protons is always the same. An atom's atomic number is equal to the number of protons in that atom. Protons are made of quarks. A proton is believed to be made up of 3 quarks, two up quarks and one down quark. One down quark has a charge of -1/3, and two up quarks have a charge of +2/3 each. This adds to a charge of +1. A proton has a very small mass. The mass of the proton is about one atomic mass unit. The mass of the neutron is also about one atomic mass unit. The size of a proton is determined by the vibration of the quarks that are in it, and these quarks effectively form a cloud. This means that a proton is not so much a hard ball as an area that contains quarks."
108,Nuclear Fusion,"The process by which two nuclei collide and coelesce to form a single, heavier nucleus.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Nuclear fusion is the process of making a single heavy nucleus (part of an atom) from two lighter nuclei. This process is called a nuclear reaction. It releases a large amount of energy. The nucleus made by fusion is heavier than either of the starting nuclei. However, it is not as heavy as the combination of the original mass of the starting nuclei (atoms). This lost mass is changed into lots of energy. This is shown in Einstein's famous E=mc2 equation. Fusion happens in the middle of stars, like the Sun. Hydrogen atoms are fused together to make helium. This releases lots of energy. This energy powers the heat and light of the star. Not all elements can be joined. Heavier elements are less easily joined than lighter ones. Iron (a metal) cannot fuse with other atoms. This is what causes stars to die. Stars join all of their atoms together to make heavier atoms of different types, until they start to make iron. The iron nucleus cannot fuse with other nuclei. The reactions stop. The star eventually will cool down and die. On Earth it is very difficult to start nuclear fusion reactions that release more energy than is needed to start the reaction. The reason is that fusion reactions only happen at high temperature and pressure, like in the Sun,because both nuclei have a positive charge, and positive repels positive. The only way to stop the repulsion is to make the nuclei hit each other at very high speeds. They only do that at high pressure and temperature. The only successful approach so far has been in nuclear weapons. The hydrogen bomb uses an atomic (fission) bomb to start fusion reactions. Scientists and engineers have been trying for decades to find a safe and working way of controlling and containing fusion reactions to generate electricity. They still have many challenges to overcome before fusion power can be used as a clean source of energy."
109,Photon,A quantum of light.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Photons (from Greek φως, meaning light), in many atomic models in physics, are particles which transmit light. In other words, light is carried over space by photons. Photon is an elementary particle that is its own antiparticle. In quantum mechanics each photon has a characteristic quantum of energy that depends on frequency: A photon associated with light at a higher frequency will have more energy (and be associated with light at a shorter wavelength). Photons have a rest mass of 0 (zero). However, Einstein's theory of relativity says that they do have a certain amount of momentum. Before the photon got its name, Einstein revived the proposal that light is separate pieces of energy (particles). These particles came to be known as photons."
110,Positron,The positively charged antiparticle of the electron.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A positron is the antimatter version of an electron. It has the same mass and spin as an electron. However, it has a positive charge, whereas an electron has a negative charge. Like all antimatter, when it meets its so-called counter partner, each are annihilated and turned into energy. The electron and positron disappear, and the total mass decreases. There is no name for this type of energy, as it is neither mechanical, radiation, chemical, electrical, nuclear, nor thermal. Although the energy does emit light photons when annihilated, it is converted from some form of energy that has not been named. A positron also very rarely makes a structure called positronium. Positronium is like an atom in many ways, but is very unstable, and usually quickly annihilates."
111,Singularity,A point in space-time at which the space-time curvature becomes infinite.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A gravitational singularity (sometimes called a spacetime singularity) is a term used to describe the center of a black hole where gravity is thought to approach infinity. In the center of each black hole is a singularity, a point where infinite density develops as spacetime approaches it. Spacetime goes toward infinite curvature and matter is crushed to infinite density under the pull of infinite gravity. At a singularity, space and time cease to exist as we know them and current laws of physics cannot be applied to this region. Singularity form by a collapse of a star, where star with high enough mass (above 30 times the sun) would shrink under its own gravity and force until a single, one dimensional point. When it forms, space and time would be infinite in there. An example would be that if a person is to stand on a collapsing star right before singularity form, and he sends a signal every second to a nearby observer, time and space would slow down as singularity is being formed, so that the observer would hear the signal slowing down. Let's say the singularity form at 12:00 exact, the observer would receive all the signals, one after another, with each successive signal slowing down at a very small rate, until the 12:00 signal, where time and space would be infinite to send a signal, in other words, forever. To understand singularity, one would first have to believe infinite density, like how the original Big Bang started. Big Bang, most scientists believe, started from an earlier universe that was pulled together by gravity to a single point, at which it was then exploded to all the matter in the expanding universe. A black hole singularity is similar, except on a smaller scale. Imagine our sun decrease to size of the nucleus of an atom, it's very dense out there. However, our sun does not have high enough mass and is well below the Chandrasekhar limit, which means it will shrink to a white dwarf, with radius of a few thousand miles."
112,Special Relativity,"Einstein's theory based on the idea that the laws of science should be the same for all observers, no matter how they are moving.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Special relativity (or the special theory of relativity) is a theory in physics that was developed and explained by Albert Einstein in 1905. It applies to all physical phenomena, so long as gravitation is not significant. Special relativity applies to Minkowski space, or ""flat spacetime"" (phenomena which are not influenced by gravitation). Einstein knew that some weaknesses had been discovered in older physics. For example, older physics thought light moved in luminiferous aether. Various tiny effects were expected if this theory were true. Gradually it seemed these predictions were not going to work out. Eventually, Einstein (1905) drew the conclusion that the concepts of space and time needed a fundamental revision. The result was special relativity theory. This is based on the constancy of the speed of light in all inertial frames of reference and the principle of relativity. inertial frame of reference: a frame of reference that describes time and space homogeneously, isotropically, and in a time-independent manner. Shorthand: space the same everywhere at all times. principle of relativity: the equations describing the laws of physics have the same form in all frames of reference. Shorthand: same equations work everywhere and at all times. Galileo had already established the principle of relativity, which said that physical events must look the same to all observers, and no observer has the ""right"" way to look at the things studied by physics. For example, the Earth is moving very fast around the Sun, but we do not notice it because we are moving with the Earth at the same speed; therefore, from our point of view, the Earth is at rest. However, Galileo's math could not explain some things, such as the speed of light. According to him, the measured speed of light should be different for different speeds of the observer in comparison with its source. However, the Michelson-Morley experiment showed that this is not true, at least not for all cases. Einstein's theory of special relativity explained this among other things."
113,Spin,"An internal property of elementary particles, related to the everyday concept of spin.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Experiments such as the Stern-Gerlach experiment have shown that sub-atomic particles, such as electrons, seem to have a north pole and a south pole much like magnets do. Scientists once thought that this was caused by the particle spinning on its axis like a planet. Later, it was shown that the electron would have to be spinning faster than the speed of light to do this. This is why scientists no longer believe that the electron is actually spinning like a planet. Scientists do, however, continue to refer to the magnetic properties of particles as ""spin"". Spin, whatever it is, seems to follow some of the laws of angular momentum, but not all of them. A ""spinning"" electron (or any other sub-atomic particle with spin) can only have certain values of angular momentum. Electrons can also align themselves against a magnetic field in ways that would be impossible in the everyday world."
114,Wormhole,A thin tupe of space-time connecting distant regions of the universe. Wormholes might also link to parallel or baby universes and could provide the possibility of timetravel.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A wormhole is a theoretical connection between a black hole and a white hole creating a shortcut through time and space. It is not known if wormholes exist in nature. Scientists believe that if wormholes existed they could not be made following any traditional scientific methods. In order to hold a wormhole open, a form of theoretical exotic matter would be needed. Otherwise the wormhole would simply disappear very quickly after its creation. If plotted on a 2-dimensional plane, the wormhole bends the plane, like folding a paper, so that the two ends would be touching. The term wormhole was first used by John Wheeler, a theoretical physicist. It is also known as an Einstein-Rosen bridge. A wormhole is much like a tunnel with two ends each in separate points in space time. Researchers have no observational evidence for wormholes."
115,Wavelength,"For a wave, the distance between two adjacent crests.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","A wavelength is the length of the shortest repeating part of a ""sine wave"". All waves can be formed by adding up sine waves. That is, every wave is a total of sine waves, which may be identified by Fourier analysis."
116,Arrow of Time,"Direction in which time seems to point, from past to future.","Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","The arrow of time, or time's arrow, is the concept of the ""one-way direction"" or ""asymmetry"" of time. It was developed in 1927 by the British astronomer Arthur Eddington, and is an unsolved general physics question. This direction, according to Eddington, can be determined by studying the organization of atoms, molecules, and bodies, and might be drawn upon a four-dimensional relativistic map of the world (""a solid block of paper""). Physical processes at the microscopic level are believed to be either entirely or mostly time-symmetric: if the direction of time were to reverse, the theoretical statements that describe them would remain true. Yet at the macroscopic level it often appears that this is not the case: there is an obvious direction (or flow) of time."
117,Classical Physics,"The physical laws of Newton and Maxwell. More generally, often used to refer to all nonquantum laws of physics.","Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","Classical physics refers to theories of physics that predate modern, more complete, or more widely applicable theories. If a currently accepted theory is considered to be modern, and its introduction represented a major paradigm shift, then the previous theories, or new theories based on the older paradigm, will often be referred to as belonging to the realm of ""classical physics"". As such, the definition of a classical theory depends on context. Classical physical concepts are often used when modern theories are unnecessarily complex for a particular situation. Most usually classical physics refers to pre-1900 physics, while modern physics refers to post-1900 physics which incorporates elements of quantum mechanics and relativity."
118,Copenhagen Interpretation,Interpretation of quantum mechanics that envisions large objects as being subject to classical laws and small objects as subject to quantum laws.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","The Copenhagen interpretation is an expression of the meaning of quantum mechanics that was largely devised in the years 1925 to 1927 by Niels Bohr and Werner Heisenberg. It remains one of the most commonly taught interpretations of quantum mechanics. According to the Copenhagen interpretation, physical systems generally do not have definite properties prior to being measured, and quantum mechanics can only predict the probabilities that measurements will produce certain results. The act of measurement affects the system, causing the set of probabilities to reduce to only one of the possible values immediately after the measurement. This feature is known as wave function collapse. There have been many objections to the Copenhagen interpretation over the years. These include: discontinuous jumps when there is an observation, the probabilistic element introduced upon observation, the subjectiveness of requiring an observer, the difficulty of defining a measuring device, and the necessity of invoking classical physics to describe the ""laboratory"" in which the results are measured. Alternatives to the Copenhagen interpretation include the many-worlds interpretation, the De Broglie–Bohm (pilot-wave) interpretation, quantum Bayesianism, and quantum decoherence theories."
119,Event Horizon,Imaginary sphere surrounding a black hole delianating the points of no return; anything crossing the event horizon cannot escape the black hole's gravity.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In general relativity, an event horizon (EH) is a region in spacetime beyond which events cannot affect an outside observer. In layman's terms, it is defined as the shell of ""points of no return"", i.e., the boundary at which the gravitational pull of a massive object becomes so great as to make escape impossible. An event horizon is most commonly associated with black holes. Light emitted from inside the event horizon can never reach the outside observer. Likewise, any object approaching the horizon from the observer's side appears to slow down and never quite pass through the horizon, with its image becoming more and more redshifted as time elapses. This means that the wavelength of the light emitted from the object is getting longer as the object moves away from the observer. The traveling object, however, experiences no strange effects and does, in fact, pass through the horizon in a finite amount of proper time. More specific types of horizon include the related but distinct absolute and apparent horizons found around a black hole. Still other distinct notions include the Cauchy and Killing horizons; the photon spheres and ergospheres of the Kerr solution; particle and cosmological horizons relevant to cosmology; and isolated and dynamical horizons important in current black hole research."
120,Dark Energy,Hypothetical pressure uniformly filling space; more general notion than a cosmological constant allowing variance for time.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In physical cosmology and astronomy, dark energy is an unknown form of energy which is hypothesized to permeate all of space, tending to accelerate the expansion of the universe. Dark energy is the most accepted hypothesis to explain the observations since the 1990s indicating that the universe is expanding at an accelerating rate. Assuming that the standard model of cosmology is correct, the best current measurements indicate that dark energy contributes 68% of the total energy in the present-day observable universe. The mass–energy of dark matter and ordinary (baryonic) matter contribute 27% and 5%, respectively, and other components such as neutrinos and photons contribute a very small amount. The density of dark energy (~ 7 × 10−30 g/cm3) is very low, much less than the density of ordinary matter or dark matter within galaxies. However, it dominates the mass–energy of the universe because it is uniform across space. Two proposed forms for dark energy are the cosmological constant, representing a constant energy density filling space homogeneously, and scalar fields such as quintessence or moduli, dynamic quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to the zero-point radiation of space i.e. the vacuum energy. Scalar fields that change in space can be difficult to distinguish from a cosmological constant because the change may be extremely slow."
121,Electromagnetic Force,One of nature's four fundemental fources; acts on particles having an electric charge.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In 1600, William Gilbert proposed, in his De Magnete, that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752. One of the first to discover and publish a link between man-made electric current and magnetism was Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to produce a theory of electromagnetism that set the subject on a mathematical foundation. A theory of electromagnetism, known as classical electromagnetism, was developed by various physicists during the period between 1820 and 1873 when it culminated in the publication of a treatise by James Clerk Maxwell, which unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law. One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.) In addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term ""electromagnetism"". "
122,Graviton,Hypothetical messanger particles of the gravitational force.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In theories of quantum gravity, the graviton is the hypothetical elementary particle that mediates the force of gravity. There is no complete quantum field theory of gravitons due to an outstanding mathematical problem with renormalization in general relativity. In string theory, believed to be a consistent theory of quantum gravity, the graviton is a massless state of a fundamental string. If it exists, the graviton is expected to be massless because the gravitational force is very long range and appears to propagate at the speed of light. The graviton must be a spin-2 boson because the source of gravitation is the stress–energy tensor, a second-order tensor (compared with electromagnetism's spin-1 photon, the source of which is the four-current, a first-order tensor). Additionally, it can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field would couple to the stress–energy tensor in the same way that gravitational interactions do. This result suggests that, if a massless spin-2 particle is discovered, it must be the graviton."
123,Electroweak Higgs Field,"Field that acquires nonzero value in cold, empty space; gives rise to masses for fundemental particles.","Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","In the Standard Model of particle physics, the Higgs mechanism is essential to explain the generation mechanism of the property ""mass"" for gauge bosons. Without the Higgs mechanism, all bosons (one of the two classes of particles, the other being fermions) would be considered massless, but measurements show that the W+, W−, and Z bosons actually have relatively large masses of around 80 GeV/c2. The Higgs field resolves this conundrum. The simplest description of the mechanism adds a quantum field (the Higgs field) that permeates all space to the Standard Model. Below some extremely high temperature, the field causes spontaneous symmetry breaking during interactions. The breaking of symmetry triggers the Higgs mechanism, causing the bosons it interacts with to have mass. In the Standard Model, the phrase ""Higgs mechanism"" refers specifically to the generation of masses for the W±, and Z weak gauge bosons through electroweak symmetry breaking. The Large Hadron Collider at CERN announced results consistent with the Higgs particle on 14 March 2013, making it extremely likely that the field, or one like it, exists, and explaining how the Higgs mechanism takes place in nature. The mechanism was proposed in 1962 by Philip Warren Anderson, following work in the late 1950s on symmetry breaking in superconductivity and a 1960 paper by Yoichiro Nambu that discussed its application within particle physics. A theory able to finally explain mass generation without ""breaking"" gauge theory was published almost simultaneously by three independent groups in 1964: by Robert Brout and François Englert; by Peter Higgs; and by Gerald Guralnik, C. R. Hagen, and Tom Kibble. The Higgs mechanism is therefore also called the Brout-Englert-Higgs mechanism, or Englert-Brout-Higgs-Guralnik-Hagen-Kibble mechanism, Anderson-Higgs mechanism, Anderson-Higgs-Kibble mechanism, Higgs-Kibble mechanism by Abdus Salam and ABEGHHK'tH mechanism [for Anderson, Brout, Englert, Guralnik, Hagen, Higgs, Kibble, and 't Hooft] by Peter Higgs. On 8 October 2013, following the discovery at CERN's Large Hadron Collider of a new particle that appeared to be the long-sought Higgs boson predicted by the theory, it was announced that Peter Higgs and François Englert had been awarded the 2013 Nobel Prize in Physics."
124,Rapid Elasticity,Refers to the sharing pooled resources to enable horizontal scalability.,"Nayan B. Ruparelia, Cloud Computing, 17 (MIT Press, 2016).","In cloud computing, elasticity is defined as ""the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible"". Elasticity is a defining characteristic that differentiates cloud computing from previously proposed computing paradigms, such as grid computing. The dynamic adaptation of capacity, e.g., by altering the use of computing resources, to meet a varying workload is called ""elastic computing""."
125,Many Worlds Interpretation,Interpretation of quantum mechanics in which all potentialities embodied by a probability wave are realized in serperate universes.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","The many-worlds interpretation is an interpretation of quantum mechanics that asserts the objective reality of the universal wavefunction and denies the actuality of wavefunction collapse. Many-worlds implies that all possible alternate histories and futures are real, each representing an actual ""world"" (or ""universe""). In layman's terms, the hypothesis states there is a very large—perhaps infinite—number of universes, and everything that could possibly have happened in our past, but did not, has occurred in the past of some other universe or universes. The theory is also referred to as MWI, the relative state formulation, the Everett interpretation, the theory of the universal wavefunction, many-universes interpretation, multiverse theory or just many-worlds. The original relative state formulation is due to Hugh Everett in 1957. Later, this formulation was popularized and renamed many-worlds by Bryce Seligman DeWitt in the 1960s and 1970s. The decoherence approaches to interpreting quantum theory have been further explored and developed, becoming quite popular. MWI is one of many multiverse hypotheses in physics and philosophy. It is currently considered a mainstream interpretation along with the other decoherence interpretations, collapse theories (including the historical Copenhagen interpretation), and hidden variable theories such as the Bohmian mechanics. Before many-worlds, reality had always been viewed as a single unfolding history. Many-worlds, however, views historical reality as a many-branched tree, wherein every possible quantum outcome is realised. Many-worlds reconciles the observation of non-deterministic events, such as random radioactive decay, with the fully deterministic equations of quantum physics. In many-worlds, the subjective appearance of wavefunction collapse is explained by the mechanism of quantum decoherence, and this is supposed to resolve all of the correlation paradoxes of quantum theory, such as the EPR paradox and Schrödinger's cat, since every possible outcome of every event defines or exists in its own ""history"" or ""world""."
126,Cosmic Horizon,Locations in space-time beyond which light has not had time to reach humans since the begining of the universe.,"Brian Greene, The Fabric of The Cosmos (Vintage Books, 2004).","A cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons."
127,Backpropagation,"A learning algorithm for artificial neural networks used for supervised learning, where connection weights are iteratively updated to decrease the approximation at the error outputs.","Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. Backpropagation is shorthand for ""the backward propagation of errors,"" since an error is computed at the output and distributed backwards throughout the network’s layers. It is commonly used to train deep neural networks. Backpropagation is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm and is part of continuing research in neural backpropagation. Backpropagation is a special case of a more general technique called automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function."
128,Class,A set of instances with the same identity.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. In some languages, classes are only a compile-time feature (new classes cannot be declared at runtime), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type Class or similar). In these languages, a class that creates classes is called a metaclass."
129,Classification,The assignment of a given instance to a set of classes.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In set theory and its applications throughout mathematics, a class is a collection of sets (or sometimes other mathematical objects) that can be unambiguously defined by a property that all its members share. The precise definition of ""class"" depends on foundational context. In work on Zermelo–Fraenkel set theory, the notion of class is informal, whereas other set theories, such as von Neumann–Bernays–Gödel set theory, axiomatize the notion of ""proper class"", e.g., as entities that are not members of another entity. A class that is not a set (informally in Zermelo–Fraenkel) is called a proper class, and a class that is a set is sometimes called a small class. For instance, the class of all ordinal numbers, and the class of all sets, are proper classes in many formal systems. Outside set theory, the word ""class"" is sometimes used synonymously with ""set"". This usage dates from a historical period where classes and sets were not distinguished as they are in modern set-theoretic terminology. Many discussions of ""classes"" in the 19th century and earlier are really referring to sets, or perhaps to a more ambiguous concept."
130,Clustering,Grouping of several instances into clusters.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","This is an unsupervised learning method because the instances that form a cluster are found based on their similarity to each other, as opposed to a classification task where the supervisor assigns instances to classes by explicitly labeling them."
131,Data Mining,Machine learning and statistical methods for extracting information from large amounts of data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. The difference between data analysis and data mining is that data analysis is to summarize the history such as analyzing the effectiveness of a marketing campaign, in contrast, data mining focuses on using specific machine learning and statistical models to predict the future and discover the patterns among data. The term ""data mining"" is in fact a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and machine learning – are more appropriate. The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps. The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations."
132,Database,Software for storing and processing digitally represented information efficiently.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques. The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a ""database system"". Often the term ""database"" is also used to loosely refer to any of the DBMS, the database system or an application associated with the database. Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages."
133,Decision Tree,A hierarchical model composed of decision nodes and leaves.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","The decision tree works fast, and it can be converted to a set of if-then rules, and as such allows knowledge extraction. Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making). This page deals with decision trees in data mining."
134,Feature Extraction,"A method of dimensionality reductions, several originial inputs are combined to define new, more informative features.","Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is a dimensionality reduction process, where an initial set of raw variables is reduced to more manageable groups (features) for processing, while still accurately and completely describing the original data set. When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data."
135,Feature Selection,A method discarding uninformative features and keeping informative features.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons: simplification of models to make them easier to interpret by researchers/users, shorter training times, to avoid the curse of dimensionality, enhanced generalization by reducing overfitting (formally, reduction of variance) The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated. Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples."
136,Model,A template formalizing the relationship between input and output.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Scientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, and graphical models to visualize the subject. Modelling is an essential and inseparable part of many scientific disciplines, each of which have their own ideas about specific types of modelling. There is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling."
137,Latent Semantic Analysis,A learning method where the aim is to find a small set of hidden variables representing the dependencies in a large sample of observed data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words. An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI)."
138,Generalization,A measure of model performance on new unseen data relative to training data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","A generalization (or generalisation) is the formulation of general concepts from specific instances by abstracting common properties. Generalizations posit the existence of a domain or set of elements, as well as one or more common characteristics shared by those elements (thus creating a conceptual model). As such, they are the essential basis of all valid deductive inferences. The process of verification is necessary to determine whether a generalization holds true for any given situation. Generalization is the process of identifying the parts of a whole, as belonging to the whole. The parts, completely unrelated may be brought together as a group, belonging to the whole by establishing a common relation between them. It must be stated that, the parts cannot be generalized into a whole until a common relation is established among all the parts. But this does not mean that the parts are unrelated, only that no common relation has been established yet for the generalization. The concept of generalization has broad application in many connected disciplines, sometimes having a specialized context or meaning. Of any two related concepts, such as A and B, A is a ""generalization"" of B, and B is a special case of A, if and only if every instance of concept B is also an instance of concept A; and there are instances of concept A which are not instances of concept B."
139,Generative Model,A model defined to represent the way data is thought to have been generated.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguishes two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model. Standard examples of each, all of which are linear classifiers, are: generative classifiers: naive Bayes classifier and linear discriminant analysis; discriminative model: logistic regression; non-model classifier: perceptron and support vector machine. In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, {\displaystyle P(Y|X=x)} {\displaystyle P(Y|X=x)} (discriminative model), and base classification on that; or one can estimate the joint distribution {\displaystyle P(X,Y)} {\displaystyle P(X,Y)} (generative model), from that compute the conditional probability {\displaystyle P(Y|X=x)} {\displaystyle P(Y|X=x)}, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches."
140,Data Warehouse,"A subset of data selected, extracted, and organized for a specific data analysis task.","Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise. The data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the DW for reporting. The typical extract, transform, load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups, often called dimensions, and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data. The main source of the data is cleansed, transformed, catalogued, and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform, and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform, and load data into the repository, and tools to manage and retrieve metadata."
141,Outlier Detection,A program for detecting anomalies in data developed to identify exceiptions to general rules.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the learnt model."
142,Parallel Distributed Processing,"A computational paradigm where the task is divided into smaller concurrent tasks, each of which can be run on a different processor.","Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","The prevailing connectionist approach today was originally known as parallel distributed processing (PDP). It was an artificial neural network approach that stressed the parallel nature of neural processing, and the distributed nature of neural representations."
143,Pattern Recognition,The identification of particular configurations of data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Pattern recognition is the automated recognition of patterns and regularities in data. Pattern recognition is closely related to artificial intelligence and machine learning, together with applications such as data mining and knowledge discovery in databases (KDD), and is often used interchangeably with these terms. However, these are distinguished: machine learning is one approach to pattern recognition, while other approaches include hand-crafted (not learned) rules or heuristics; and pattern recognition is one approach to artificial intelligence, while other approaches include symbolic artificial intelligence. A modern definition of pattern recognition is: The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. This article focuses on machine learning approaches to pattern recognition. Pattern recognition systems are in many cases trained from labeled ""training"" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). Machine learning is the common term for supervised learning methods [dubious – discuss] and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is not generally a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms."
144,Perceptron,A type of neural network organized into layers where each layer receives connections from units in the previous layer and feeds its output to the units of the following layer.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
145,Regression,Estimating a numeric value for a given instance.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, a function of the independent variables called the regression function is to be estimated. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the regression function using a probability distribution. A related but distinct approach is Necessary Condition Analysis (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is necessary but not sufficient for a given value of the dependent variable. Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable. Many techniques for carrying out regression analysis have been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional. The performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results. In a narrower sense, regression may refer specifically to the estimation of continuous response (dependent) variables, as opposed to the discrete response variables used in classification. The case of a continuous dependent variable may be more specifically referred to as metric regression to distinguish it from related problems."
146,Sample,A set of observed data.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In statistics and quantitative research methodology, a data sample is a set of data collected and/or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Typically, the population is very large, making a census or a complete enumeration of all the values in the population either impractical or impossible. The sample usually represents a subset of manageable size. Samples are collected and statistics are calculated from the samples, so that one can make inferences or extrapolations from the sample to the population. The data sample may be drawn from a population without replacement (i.e. no element can be selected more than once in the same sample), in which case it is a subset of a population; or with replacement (i.e. an element may appear multiple times in the one sample), in which case it is a multisubset."
147,Validation,The process of testing the generalization performance of a trained model by testing it on data unseen during training.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","In computer science, data validation is the process of ensuring data have undergone data cleansing to ensure they have data quality, that is, that they are both correct and useful. It uses routines, often called ""validation rules"" ""validation constraints"" or ""check routines"", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic."
148,Web Scraping,Software automatically surfing the web and extracting information.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis. Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping). Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration. Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an Application Programming Interface (API) to extract data from a web site. Companies like Amazon AWS and Google provide web scraping tools, services and public data available free of cost to end users. Newer forms of web scraping involve listening to data feeds from web servers. For example, JSON is commonly used as a transport storage mechanism between the client and the web server. There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing."
149,Computer,"A machine with electronic components performing calculations, storring data, and decoding program instructions.","Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A computer is a device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks. Computers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones. Early computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers have been increasing dramatically ever since then. Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved."
150,Data,Information about the world.,"Ethem Alapaydin, Machine Learning (The MIT Press, 2016).","Data and information are often used interchangeably; however data becomes information when it is viewed in context or in post-analysis. While the concept of data is commonly associated with scientific research, data is collected by a huge range of organizations and institutions, including businesses (e.g., sales data, revenue, profits, stock price), governments (e.g., crime rates, unemployment rates, literacy rates) and non-governmental organizations (e.g., censuses of the number of homeless people by non-profit organizations). Data is measured, collected and reported, and analyzed, whereupon it can be visualized using graphs, images or other analysis tools. Data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing. Raw data (""unprocessed data"") is a collection of numbers or characters before it has been ""cleaned"" and corrected by researchers. Raw data needs to be corrected to remove outliers or obvious instrument or data entry errors (e.g., a thermometer reading from an outdoor Arctic location recording a tropical temperature). Data processing commonly occurs by stages, and the ""processed data"" from one stage may be considered the ""raw data"" of the next stage. Field data is raw data that is collected in an uncontrolled ""in situ"" environment. Experimental data is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new oil of the digital economy."
151,Electromechanical,A method of manipulating electrical circuits using metal contacts activated by electrical currents.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","In engineering, electromechanics combines processes and procedures drawn from electrical engineering and mechanical engineering. Electromechanics focuses on the interaction of electrical and mechanical systems as a whole and how the two systems interact with each other. This process is especially prominent in systems such as those of DC Machines which can be designed and operated to generate power from a mechanical process (generator) or used to power a mechanical effect (motor). Electrical engineering in this context also encompasses electronics engineering. Electromechanical devices are ones which have both electrical and mechanical processes. Strictly speaking, a manually operated switch is an electromechanical component due to the mechanical movement causing an electrical output. Though this is true, the term is usually understood to refer to devices which involve an electrical signal to create mechanical movement, or vice versa mechanical movement to create an electric signal. Often involving electromagnetic principles such as in relays, which allow a voltage or current to control another, usually isolated circuit voltage or current by mechanically switching sets of contacts, and solenoids, by which a voltage can actuate a moving linkage as in solenoid valves. Before the development of modern electronics, electromechanical devices were widely used in complicated subsystems of parts, including electric typewriters, teleprinters, clocks, initial television systems, and the very early electromechanical digital computers."
152,Electronic,A method of switching using electrons moving at high speeds.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","Electronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter. The identification of the electron in 1897, along with the invention of the vacuum tube, which could amplify and rectify small electrical signals, inaugurated the field of electronics and the electron age. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, integrated circuits, optoelectronics, and sensors, associated passive electrical components, and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible. Electronics is widely used in information processing, telecommunication, and signal processing. The ability of electronic devices to act as switches makes digital information-processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system. Electrical and electromechanical science and technology deals with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms (using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components). This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device. Until 1950 this field was called ""radio technology"" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes. As of 2018 most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. This article focuses on engineering aspects of electronics."
153,Internet,Worldwide network using TCP/IP protocols and conforming the addessing of the Domain Name System.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","The Internet (contraction of interconnected network) is the global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing. The origins of the Internet date back to research commissioned by the federal government of the United States in the 1960s to build robust, fault-tolerant communication with computer networks. The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1980s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks. The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet, and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia since the 1980s, commercialization incorporated its services and technologies into virtually every aspect of modern life. Most traditional communications media, including telephony, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephony, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking. Online shopping has grown exponentially both for major retailers and small businesses and entrepreneurs, as it enables firms to extend their ""brick and mortar"" presence to serve a larger market or even sell goods and services entirely online. Business-to-business and financial services on the Internet affect supply chains across entire industries. The Internet has no centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies. Only the overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. In November 2006, the Internet was included on USA Today's list of New Seven Wonders."
154,Microprocessor,A device containing most of the basic components of a general-purpose stored program computer on a single chip.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A microprocessor is a computer processor that incorporates the functions of a central processing unit on a single integrated circuit (IC), or at most a few integrated circuits. The microprocessor is a multipurpose, clock driven, register based, digital integrated circuit that accepts binary data as input, processes it according to instructions stored in its memory, and provides results as output. Microprocessors contain both combinational logic and sequential digital logic. Microprocessors operate on numbers and symbols represented in the binary number system. The integration of a whole CPU onto a single chip or on a few chips greatly reduced the cost of processing power, increasing efficiency. Integrated circuit processors are produced in large numbers by highly automated processes, resulting in a low per-unit cost. Single-chip processors increase reliability because there are many fewer electrical connections that could fail. As microprocessor designs improve, the cost of manufacturing a chip (with smaller components built on a semiconductor chip the same size) generally stays the same according to Rock's law. Before microprocessors, small computers had been built using racks of circuit boards with many medium- and small-scale integrated circuits. Microprocessors combined this into one or a few large-scale ICs. Continued increases in microprocessor capacity have since rendered other forms of computers almost completely obsolete (see history of computing hardware), with one or more microprocessors used in everything from the smallest embedded systems and handheld devices to the largest mainframes and supercomputers."
155,Memory,The part of a computer where data is stored.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","Memory is the faculty of the brain by which information is encoded, stored, and retrieved when needed. Memory is vital to experiences, it is the retention of information over time for the purpose of influencing future action. If we could not remember past events, we could not learn or develop language, relationships, or personal identity (Eysenck, 2012). Often memory is understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory (Baddely, 2007). This can be related to the neuron. The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to with various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store data through various categorical models or systems (Baddely, 2007). Explicit and implicit functions of memory are also known as declarative and non-declarative systems (Squire, 2009).  These systems involve the purposeful intention of memory retrieval and storage, or lack thereof. Declarative, or explicit, memory is the conscious storage and recollection of data (Graf & Schacter, 1985). Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning (Eysenck, 2012), while episodic memory refers to information that is encoded along a spatial and temporal plane (Schacter & Addis, 2007; Szpunar, 2010). Declarative memory is usually the primary process thought of when referencing memory (Eysenck, 2012). Non-declarative, or implicit, memory is the unconscious storage and recollection of information (Foerde & Poldrack, 2009). An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon (Eysenck, 2012; Foerde & Poldrack, 2009; Tulving & Schacter, 1990). Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated (Tulving & Schacter, 1990), whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning (Eysenck, 2012; Foerde & Poldrack, 2009). Memory is not a perfect processor, and is affected by many factors. The manner information is encoded, stored, and retrieved can all be corrupted. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage (Eysenck, 2012). Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus (Squire, 2009). Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory (Eysenck, 2012). Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of memory. Memory loss is usually described as forgetfulness or amnesia."
156,Metal-oxide Semiconductor,A type of integrated circuit lending itself to high densities and lower power consumption.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","The metal-oxide-semiconductor field-effect transistor (MOSFET, MOS-FET, or MOS FET) is a type of field-effect transistor (FET), most commonly fabricated by the controlled oxidation of silicon. It has an insulated gate, whose voltage determines the conductivity of the device. This ability to change conductivity with the amount of applied voltage can be used for amplifying or switching electronic signals. A metal-insulator-semiconductor field-effect transistor or MISFET is a term almost synonymous with MOSFET. Another synonym is IGFET for insulated-gate field-effect transistor. The basic principle of the field-effect transistor was first patented by Julius Edgar Lilienfeld in 1925. The main advantage of a MOSFET is that it requires almost no input current to control the load current, when compared with bipolar transistors. In an enhancement mode MOSFET, voltage applied to the gate terminal increases the conductivity of the device. In depletion mode transistors, voltage applied at the gate reduces the conductivity. The ""metal"" in the name MOSFET is now often a misnomer because the gate material is often a layer of polysilicon (polycrystalline silicon). Similarly, ""oxide"" in the name can also be a misnomer, as different dielectric materials are used with the aim of obtaining strong channels with smaller applied voltages. The MOSFET is by far the most common transistor in digital circuits, as billions may be included in a memory chip or microprocessor. Since MOSFETs can be made with either p-type or n-type semiconductors, complementary pairs of MOS transistors can be used to make switching circuits with very low power consumption, in the form of CMOS logic."
157,Protocols,The conventions governing switching packets. Analogous to the address of a letter.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","In telecommunication, a communication protocol is a system of rules that allow two or more entities of a communications system to transmit information via any kind of variation of a physical quantity. The protocol defines the rules, syntax, semantics and synchronization of communication and possible error recovery methods. Protocols may be implemented by hardware, software, or a combination of both. Communicating systems use well-defined formats for exchanging various messages. Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. The specified behavior is typically independent of how it is to be implemented. Communication protocols have to be agreed upon by the parties involved. To reach agreement, a protocol may be developed into a technical standard. A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communication what programming languages are to computations. Multiple protocols often describe different aspects of a single communication. A group of protocols designed to work together are known as a protocol suite; when implemented in software they are a protocol stack. Internet communication protocols are published by the Internet Engineering Task Force (IETF). The IEEE handles wired and wireless networking, and the International Organization for Standardization (ISO) handles other types. The ITU-T handles telecommunication protocols and formats for the public switched telephone network (PSTN). As the PSTN and Internet converge, the standards are also being driven towards convergence."
158,Packet Switching,A method of transmitting data over a computer network by breaking up a file into smaller subsections called packets.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","Packet switching is a method of grouping data that is transmitted over a digital network into packets. Packets are made of a header and a payload. Data in the header are used by networking hardware to direct the packet to its destination where the payload is extracted and used by application software. Packet switching is the primary basis for data communications in computer networks worldwide. In the early 1960s, American computer scientist Paul Baran developed the concept Distributed Adaptive Message Block Switching with the goal to provide a fault-tolerant, efficient routing method for telecommunication messages as part of a research program at the RAND Corporation, funded by the US Department of Defense. This concept contrasted and contradicted then-established principles of pre-allocation of network bandwidth, largely fortified by the development of telecommunications in the Bell System. The new concept found little resonance among network implementers until the independent work of British computer scientist Donald Davies at the National Physical Laboratory (United Kingdom) in 1965. Davies is credited with coining the modern term packet switching and inspiring numerous packet switching networks in the decade following, including the incorporation of the concept in the early ARPANET in the United States."
159,Silicon,An element which has properties well suited for the construction of integrated circuits.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","Silicon is a chemical element with symbol Si and atomic number 14. It is a hard and brittle crystalline solid with a blue-grey metallic lustre; and it is a tetravalent metalloid and semiconductor. It is a member of group 14 in the periodic table: carbon is above it; and germanium, tin, and lead are below it. It is relatively unreactive. Because of its large chemical affinity for oxygen, it was not until 1823 that Jöns Jakob Berzelius was first able to prepare it and characterize it in pure form. Its melting and boiling points of 1414 °C and 3265 °C respectively are the second-highest among all the metalloids and nonmetals, being only surpassed by boron. Silicon is the eighth most common element in the universe by mass, but very rarely occurs as the pure element in the Earth's crust. It is most widely distributed in dusts, sands, planetoids, and planets as various forms of silicon dioxide (silica) or silicates. More than 90% of the Earth's crust is composed of silicate minerals, making silicon the second most abundant element in the Earth's crust (about 28% by mass) after oxygen. Most silicon is used commercially without being separated, and often with little processing of the natural minerals. Such use includes industrial construction with clays, silica sand, and stone. Silicates are used in Portland cement for mortar and stucco, and mixed with silica sand and gravel to make concrete for walkways, foundations, and roads. They are also used in whiteware ceramics such as porcelain, and in traditional quartz-based soda-lime glass and many other specialty glasses. Silicon compounds such as silicon carbide are used as abrasives and components of high-strength ceramics. Silicon is the basis of the widely used synthetic polymers called silicones. Elemental silicon also has a large impact on the modern world economy. Most free silicon is used in the steel refining, aluminium-casting, and fine chemical industries (often to make fumed silica). Even more visibly, the relatively small portion of very highly purified elemental silicon used in semiconductor electronics (< 10%) is essential to integrated circuits — most computers, cell phones, and modern technology depend on it. Silicon is an essential element in biology, although only traces are required by animals. However, various sea sponges and microorganisms, such as diatoms and radiolaria, secrete skeletal structures made of silica. Silica is deposited in many plant tissues."
160,Transistor,A semiconductor that switches electronic signals.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","A transistor is a semiconductor device used to amplify or switch electronic signals and electrical power. It is composed of semiconductor material usually with at least three terminals for connection to an external circuit. A voltage or current applied to one pair of the transistor's terminals controls the current through another pair of terminals. Because the controlled (output) power can be higher than the controlling (input) power, a transistor can amplify a signal. Today, some transistors are packaged individually, but many more are found embedded in integrated circuits. The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. Julius Edgar Lilienfeld patented a field-effect transistor in 1926 but it was not possible to actually construct a working device at that time. The first practically implemented device was a point-contact transistor invented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley. The transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement. Most transistors are made from very pure silicon or germanium, but certain other semiconductor materials can also be used. A transistor may have only one kind of charge carrier, in a field effect transistor, or may have two kinds of charge carriers in bipolar junction transistor devices. Compared with the vacuum tube, transistors are generally smaller, and require less power to operate. Certain vacuum tubes have advantages over transistors at very high operating frequencies or high operating voltages. Many types of transistors are made to standardized specifications by multiple manufacturers."
161,Vacuum Tube,"A device switching electrons in a moving vacuum, excited by a hot filament.","Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","In electronics, a vacuum tube, an electron tube, or valve (British usage) or, colloquially, a tube (North America), is a device that controls electric current flow in a high vacuum between electrodes to which an electric potential difference has been applied. The type known as a thermionic tube or thermionic valve uses the phenomenon of thermionic emission of electrons from a heated cathode and is used for a number of fundamental electronic functions such as signal amplification and current rectification. Non-thermionic types, such as a vacuum phototube however, achieve electron emission through the photoelectric effect, and are used for such as the detection of light levels. In both types, the electrons are accelerated from the cathode to the anode by the electric field in the tube. The simplest vacuum tube, the diode invented in 1904 by John Ambrose Fleming, contains only a heated electron-emitting cathode and an anode. Current can only flow in one direction through the device -- from the cathode to the anode. Adding one or more control grids within the tube allows the current between the cathode and anode to be controlled by the voltage on the grid or grids. These devices became a key component of electronic circuits for the first half of the twentieth century. They were crucial to the development of radio, television, radar, sound recording and reproduction, long distance telephone networks, and analogue and early digital computers. Although some applications had used earlier technologies such as the spark gap transmitter for radio or mechanical computers for computing, it was the invention of the thermionic vacuum tube that made these technologies widespread and practical, and created the discipline of electronics. In the 1940s the invention of semiconductor devices made it possible to produce solid-state devices, which are smaller, more efficient, reliable and durable, and cheaper than thermionic tubes. From the mid-1960s, thermionic tubes were then being replaced with the transistor. However, the cathode-ray tube (CRT) remained the basis for television monitors and oscilloscopes until the early 21st century. Thermionic tubes still have some applications, such as the magnetron used in microwave ovens, and certain high-frequency amplifiers. Not all electronic circuit valves/electron tubes are vacuum tubes. Gas-filled tubes are similar devices, but containing a gas, typically at low pressure, which exploit phenomena related to electric discharge in gases, usually without a heater."
162,World Wide Web,A program available on the internet allowing those connected to the network to access information in an accessible way.,"Paul E. Ceruzzi, Computing: A Concise History (MIT Press, 2012).","The World Wide Web (WWW), also called the Web, is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and accessible via the Internet. English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN near Geneva, Switzerland. The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet. Web pages are primarily text documents formatted and annotated with Hypertext Markup Language (HTML). In addition to formatted text, web pages may contain images, video, audio, and software components that are rendered in the user's web browser as coherent pages of multimedia content. Embedded hyperlinks permit users to navigate between web pages. Multiple web pages with a common theme, a common domain name, or both, make up a website. Website content can largely be provided by the publisher, or interactively where users contribute content or the content depends upon the users or their actions. Websites may be mostly informative, primarily for entertainment, or largely for commercial, governmental, or non-governmental organizational purpose."
163,Globalization,"The integration of different world regions, societies, cultures, economies, and political systems.","Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","Globalization or globalisation is the process of interaction and integration among people, companies, and governments worldwide. As a complex and multifaceted phenomenon, globalization is considered as a form of capitalist expansion which entails the integration of local and national economies into a global, unregulated market economy. Globalization has grown due to advances in transportation and communication technology. With the increased global interactions comes the growth of international trade, ideas, and culture. Globalization is primarily an economic process of interaction and integration that's associated with social and cultural aspects. However, conflicts and diplomacy are also large parts of the history of globalization, and modern globalization. Economically, globalization involves goods and services, and the economic resources of capital, technology, and data. Also, with the expansions of global markets liberalize the economic activities of exchange of goods and funds. Removal of Cross-Border Trades barriers has made formation of Global Markets more feasible. Globalization and its Impacts on the World Economic Development. The steam locomotive, steamship, jet engine, and container ships are some of the advances in the means of transport while the rise of the telegraph and its modern offspring, the Internet and mobile phones show development in telecommunications infrastructure. All of these improvements have been major factors in globalization and have generated further interdependence of economic and cultural activities around the globe. Though many scholars place the origins of globalization in modern times, others trace its history long before the European Age of Discovery and voyages to the New World, some even to the third millennium BC. Large-scale globalization began in the 1820s. In the late 19th century and early 20th century, the connectivity of the world's economies and cultures grew very quickly. The term globalization is recent, only establishing its current meaning in the 1970s. In 2000, the International Monetary Fund (IMF) identified four basic aspects of globalization: trade and transactions, capital and investment movements, migration and movement of people, and the dissemination of knowledge. Further, environmental challenges such as global warming, cross-boundary water, air pollution, and over-fishing of the ocean are linked with globalization. Globalizing processes affect and are affected by business and work organization, economics, socio-cultural resources, and the natural environment. Academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization, and political globalization."
164,Rocket,A self-contained propulsion system in which all propellants are carried internally and force is created through the expulsion of a jet of hot gas or ionized plasma.,"Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","A rocket (from Italian rocchetto ""bobbin"") is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket before use. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space. In fact, rockets work more efficiently in space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, and/or gravity. Rockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Earth's moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration. Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellants), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react, a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks."
165,Planetization,The rising consciousness that Earth is a planet in the solar system like other planets.,"Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","A planet is an astronomical body orbiting a star or stellar remnant that is massive enough to be rounded by its own gravity, is not massive enough to cause thermonuclear fusion, and has cleared its neighbouring region of planetesimals. The term planet is ancient, with ties to history, astrology, science, mythology, and religion. Five planets in the Solar System are visible to the naked eye. These were regarded by many early cultures as divine, or as emissaries of deities. As scientific knowledge advanced, human perception of the planets changed, incorporating a number of disparate objects. In 2006, the International Astronomical Union (IAU) officially adopted a resolution defining planets within the Solar System. This definition is controversial because it excludes many objects of planetary mass based on where or what they orbit. Although eight of the planetary bodies discovered before 1950 remain ""planets"" under the modern definition, some celestial bodies, such as Ceres, Pallas, Juno and Vesta (each an object in the solar asteroid belt), and Pluto (the first trans-Neptunian object discovered), that were once considered planets by the scientific community, are no longer viewed as such. The planets were thought by Ptolemy to orbit Earth in deferent and epicycle motions. Although the idea that the planets orbited the Sun had been suggested many times, it was not until the 17th century that this view was supported by evidence from the first telescopic astronomical observations, performed by Galileo Galilei. At about the same time, by careful analysis of pre-telescopic observational data collected by Tycho Brahe, Johannes Kepler found the planets' orbits were elliptical rather than circular. As observational tools improved, astronomers saw that, like Earth, each of the planets rotated around an axis tilted with respect to its orbital pole, and some shared such features as ice caps and seasons. Since the dawn of the Space Age, close observation by space probes has found that Earth and the other planets share characteristics such as volcanism, hurricanes, tectonics, and even hydrology. Planets are generally divided into two main types: large low-density giant planets, and smaller rocky terrestrials. There are eight planets in the Solar System. In order of increasing distance from the Sun, they are the four terrestrials, Mercury, Venus, Earth, and Mars, then the four giant planets, Jupiter, Saturn, Uranus, and Neptune. Six of the planets are orbited by one or more natural satellites. Several thousands of planets around other stars (""extrasolar planets"" or ""exoplanets"") have been discovered in the Milky Way. As of 1 December 2018, 3,903 known extrasolar planets in 2,909 planetary systems (including 647 multiple planetary systems), ranging in size from just above the size of the Moon to gas giants about twice as large as Jupiter have been discovered, out of which more than 100 planets are the same size as Earth, nine of which are at the same relative distance from their star as Earth from the Sun, i.e. in the circumstellar habitable zone. On December 20, 2011, the Kepler Space Telescope team reported the discovery of the first Earth-sized extrasolar planets, Kepler-20e  and Kepler-20f, orbiting a Sun-like star, Kepler-20. A 2012 study, analyzing gravitational microlensing data, estimates an average of at least 1.6 bound planets for every star in the Milky Way. Around one in five Sun-like[b] stars is thought to have an Earth-sized[c] planet in its habitable[d] zone."
166,Liquid-propellant Rocket,A rocket propulsion system based on the chemical reaction of one or more liquid propellants.,"Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","A liquid-propellant rocket or liquid rocket is a rocket engine that uses liquid propellants. Liquids are desirable because their reasonably high density allows the volume of the propellant tanks to be relatively low, and it is possible to use lightweight centrifugal turbopumps to pump the propellant from the tanks into the combustion chamber, which means that the propellants can be kept under low pressure. This permits the use of low-mass propellant tanks, resulting in a high mass ratio for the rocket. An inert gas stored in a tank at a high pressure is sometimes used instead of pumps in simpler small engines to force the propellants into the combustion chamber. These engines may have a lower mass ratio, but are usually more reliable, and are therefore used widely in satellites for orbit maintenance. Liquid rockets can be monopropellant rockets using a single type of propellant, bipropellant rockets using two types of propellant, or more exotic tripropellant rockets using three types of propellant. Some designs are throttleable for variable thrust operation and some may be restarted after a previous in-space shutdown. Liquid propellants are also used in hybrid rockets, in which a liquid oxidizer is generally combined with a solid fuel."
167,Liquid Oxygen,The most common oxidizer used in liquid-propellant rockets.,"Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","Liquid oxygen is a common cryogenic liquid oxidizer propellant for spacecraft rocket applications, usually in combination with liquid hydrogen, kerosene or methane. Liquid oxygen was used in the very first liquid fueled rocket. The World War II V2 missile also used liquid oxygen under the name A-Stoff and Sauerstoff. In the 1950's, during the Cold War both the United States' Redstone and Atlas rockets, and the Soviet R-7 Semyorka used liquid oxygen. Later, in the 1960's & 70's, the ascent stages of the Apollo Saturn rockets, and the Space Shuttle main engines used liquid oxygen."
168,Low Earth Orbit,"Orbit trajectory near Earth defined as 100-1,200 miles above surface.","Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","A low Earth orbit (LEO) is defined by Space-Track.org as an Earth-centered orbit with at least 11.25 periods per day (an orbital period of 128 minutes or less) and an eccentricity less than 0.25. Most of the manmade objects in space are in LEO orbits. A histogram of the mean motion of the cataloged objects shows that the number of objects drops significantly beyond 11.25. There is a large variety of other sources that define LEO in terms of altitude. The altitude of an object in an elliptic orbit can vary significantly along the orbit. Even for circular orbits, the altitude above ground can vary by as much as 30 km (19 mi) (especially for polar orbits) due to the oblateness of Earth's spheroid figure and local topography. While definitions in terms of altitude are inherently ambiguous, most of them fall within the range specified by an orbit period of 128 minutes because, according to Kepler's third law, this corresponds to a semi-major axis of 8,413 km (5,228 mi). For circular orbits, this in turn corresponds to an altitude of 2,042 km (1,269 mi) above the mean radius of Earth, which is consistent with some of the upper limits in the LEO definitions in terms of altitude. The LEO region is defined by some sources as the region in space that LEO orbits occupy. Some highly elliptical orbits may pass through the LEO region near their lowest altitude (or perigee) but are not in an LEO Orbit because their highest altitude (or apogee) exceeds 2,000 km (1,200 mi). Sub-orbital objects can also reach the LEO region but are not in an LEO orbit because they re-enter the atmosphere. The distinction between LEO orbits and the LEO region is especially important for analysis of possible collisions between objects which may not themselves be in LEO but could collide with satellites or debris in LEO orbits. The International Space Station conducts operations in LEO. All crewed space stations to date, as well as the majority of satellites, have been in LEO. The altitude record for human spaceflights in LEO was Gemini 11 with an apogee of 1,374.1 km (853.8 mi). Apollo 8 was the first mission to carry humans beyond LEO on December 21–27, 1968. The Apollo program continued during the four-year period spanning 1968 through 1972 with 24 astronauts who flew lunar flights but since then there have been no human spaceflights beyond LEO."
169,Medium Earth Orbit,"An orbital region about 11,000-12,000 miles altitude, occupied primarily by navigation spacecraft.","Michael J. Neufeld, Spaceflight: A Concise History (MIT Press, 2018).","Medium Earth orbit (MEO), sometimes called intermediate circular orbit (ICO), is the region of space around Earth above low Earth orbit (altitude of 2,000 km (1,243 mi) above sea level) and below geostationary orbit (altitude of 35,786 km (22,236 mi) above sea level). The most common use for satellites in this region is for navigation, communication, and geodetic/space environment science. The most common altitude is approximately 20,200 kilometres (12,552 mi)), which yields an orbital period of 12 hours, as used, for example, by the Global Positioning System (GPS). Other satellites in medium Earth orbit include Glonass (with an altitude of 19,100 kilometres (11,868 mi)) and Galileo (with an altitude of 23,222 kilometres (14,429 mi)) constellations. Communications satellites that cover the North and South Pole are also put in MEO. The orbital periods of MEO satellites range from about 2 to nearly 24 hours. Telstar 1, an experimental satellite launched in 1962, orbited in MEO. The orbit is home to a number of artificial satellites."
170,Kernel Function,Projects data from a low-dimensional space to a higher dimensional space.,"Manon Legrand, Deep Reinforcement Learning for Autonomous Vehicle Control among Human Drivers, Universite Libre de Bruxelles, (2017).","In operator theory, a branch of mathematics, a positive definite kernel is a generalization of a positive definite function or a positive-definite matrix. It was first introduced by James Mercer in the early 20th century, in the context of solving integral operator equations. Since then positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, machine learning, embedding problem, information theory, and other areas. This article will discuss some of the historical and current developments of the theory of positive definite kernels, starting with the general idea and properties before considering practical applications."
171,Support Vector Machine,Supervised learning models associated with regression and classification.,"Sebastian Raschka, Vahid Mirjalili, Python Machine Learning (Packt 2017).","In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. When data is unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications."
172,Experience Replay,A biologically inspired mechanism that using a sample of prior actions instead of the most recent action to iteratively update Q-value towards target values.,"Volodymyr Mnih, Koray Kavukcuoglu, Methods and Apparatus for Reinforcement Learning, Google, Inc., Appl. No: 14/097,862 (Dec. 5, 2013) https://patents.google.com/patent/US20150100530A1/en.","Rules are often stochastic. The observation typically involves the scalar, immediate reward associated with the last transition. In many works, the agent is assumed to observe the current environmental state (full observability). If not, the agent has partial observability. Sometimes the set of actions available to the agent is restricted (a zero balance cannot be reduced). A reinforcement learning agent interacts with its environment in discrete time steps. The agent can (possibly randomly) choose any action as a function of the history. When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative. Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and go (AlphaGo). Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations: A model of the environment is known, but an analytic solution is not available; Only a simulation model of the environment is given (the subject of simulation-based optimization); The only way to collect information about the environment is to interact with it. The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems."
173,Double Q-Learning,"Double Q-learning is an off policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.","Hado van Hasselt, Arthur Guez, and David Silver, Deep Reinforcement Learning with Q-Learning, Google DeepMind, (Cornell University Library 2016) https://arxiv.org/abs/1509.06461.","The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games."
174,Function Approximator,"In general, a function approximator selects a function among a well-defined class closely matching a target function in a task-specific way.","Volodymyr Mnih, Koray Kavukcuoglu, Methods and Apparatus for Reinforcement Learning, Google, Inc., Appl. No: 14/097,862 (Dec. 5, 2013) https://patents.google.com/patent/US20150100530A1/en.","A function approximation problem asks us to select a function among a well-defined class that closely matches (""approximates"") a target function in a task-specific way. First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.). Second, the target function, call it g, may be unknown; instead of an explicit formula, only a set of points of the form (x, g(x)) is provided. Depending on the structure of the domain and codomain of g, several techniques for approximating g may be applicable. For example, if g is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of g is a finite set, one is dealing with a classification problem instead. To some extent, the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems."
175,Tensor,A tensor is a multi-dimensional array.,"TensorFlow, Learn (2018) https://www.tensorflow.org/tutorials/?nav=true.","In mathematics, a tensor is an arbitrarily complex geometric object that maps in a (multi-)linear manner geometric vectors, scalars, and other tensors to a resulting tensor. Thereby, vectors and scalars themselves, often used already in elementary physics and engineering applications, are considered as the simplest tensors. Additionally, vectors from the dual space of the vector space, which supplies the geometric vectors, are also included as tensors. Geometric in this context is chiefly meant to emphasize independence of any selection of a coordinate system. Elementary examples of such relations include the dot product, mapping two vectors to a tensor, which is a simple scalar. A more complex example is the Cauchy stress tensor T, which takes a directional unit vector v as input and maps it to the stress vector T(v), which is the force (per unit area) exerted by material on the negative side of the plane orthogonal to v against the material on the positive side of the plane, thus expressing a relationship between these two vectors, shown in the figure (right). The cross product, where two vectors are mapped to a third one, is strictly speaking not a tensor, because it changes its sign under those transformations that change the orientation of the coordinate system. Assuming a basis of a real vector space, a coordinate frame in the ambient space, a tensor can be represented as an organized multidimensional array of numerical values with respect to this specific basis. Changing the basis transforms the values in the array in a characteristic way that allows to define tensors as objects adhering to this transformational behavior. For example, there are invariants of tensors that must be preserved under any change of the basis, thereby making only certain multidimensional arrays of numbers a tensor. Because the components of vectors and their duals transform differently under the change of their dual bases, there is a covariant and/or contravariant transformation law that relates the arrays, which represent the tensor with respect to one basis and that with respect to the other one. The numbers of, respectively, vectors: n (contravariant indices) and dual vectors: m (covariant indices) in the input and output of a tensor determine the type (or valence) of the tensor, a pair of natural numbers (n, m), which determine the precise form of the transformation law. The order of a tensor is the sum of these two numbers. The order (also degree or rank) of a tensor is thus the sum of the orders of its arguments plus the order of the resulting tensor. This is also the dimensionality of the array of numbers needed to represent the tensor with respect to a specific basis, or equivalently, the number of indices needed to label each component in that array. For example in a fixed basis, a standard linear map that maps a vector to a vector, is represented by a matrix (a 2-dimensional array), and therefore is a 2nd-order tensor. A simple vector can be represented as a 1-dimensional array, and is therefore a 1st-order tensor. Scalars are simple numbers and are thus 0th-order tensors. The collection of tensors on a vector space and its dual forms a tensor algebra, which allows products of arbitrary tensors. Simple applications of tensors of order 2, which can be represented as a square matrix, can be solved by clever arrangement of transposed vectors and by applying the rules of matrix multiplication, but the tensor product should not be confused with this. Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), or general relativity (stress–energy tensor, curvature tensor, ... ) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are simply called ""tensors"". Tensors were conceived in 1900 by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor."
176,Bellman Equation,Expresses the relationship between the value of a state and the values of its successor states.,"Mykel J. Kochenderfer, Decision Making Under Uncertainty (MIT Press 2015).","A Bellman equation, named after Richard E. Bellman, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. It writes the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices. This breaks a dynamic optimization problem into a sequence of simpler subproblems, as Bellman's “principle of optimality” prescribes. The Bellman equation was first applied to engineering control theory and to other topics in applied mathematics, and subsequently became an important tool in economic theory; though the basic concepts of dynamic programming are prefigured in John von Neumann and Oskar Morgenstern's Theory of Games and Economic Behavior and Abraham Wald's sequential analysis. Almost any problem that can be solved using optimal control theory can also be solved by analyzing the appropriate Bellman equation. However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems. In continuous-time optimization problems, the analogous equation is a partial differential equation that is usually called the Hamilton–Jacobi–Bellman equation."
177,Q-Function,Represents the expected cumulative reward from taking an action in a state and then following a policy.,"The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).","In statistics, the Q-function is the tail distribution function of the standard normal distribution. Other definitions of the Q-function, all of which are simple transformations of the normal cumulative distribution function, are also used occasionally. Because of its relation to the cumulative distribution function of the normal distribution, the Q-function can also be expressed in terms of the error function, which is an important function in applied mathematics and physics."
178,Allen-Cahn Equation,"A reaction-diffusion equation of mathematical physics which describes the process of phase separation in multi-component alloy systems, including order-disorder transitions.","Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","We present a new numerical scheme for solving a conservative Allen–Cahn equation with a space–time dependent Lagrange multiplier. Since the well-known classical Allen–Cahn equation does not have mass conservation property, Rubinstein and Sternberg introduced a nonlocal Allen–Cahn equation with a time dependent Lagrange multiplier to enforce conservation of mass. However, with their model it is difficult to keep small features since they dissolve into the bulk region. One of the reasons for this is that mass conservation is realized by a global correction using the time-dependent Lagrange multiplier. To resolve the problem, we use a space–time dependent Lagrange multiplier to preserve the volume of the system and propose a practically unconditionally stable hybrid scheme to solve the model. The numerical results indicate a potential usefulness of our proposed numerical scheme for accurately calculating geometric features of interfaces."
179,Adam Optimization Algorithm,An extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.,"Maziar Raissi, Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations (Cornell University Library 2018) https://arxiv.org/abs/1804.07010.","Adam is different to classical stochastic gradient descent. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. The authors describe Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically: Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems). Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy). Adam realizes the benefits of both AdaGrad and RMSProp. Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance). Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. The initial value of the moving averages and beta1 and beta2 values close to 1.0 (recommended) result in a bias of moment estimates towards zero. This bias is overcome by first calculating the biased estimates before then calculating bias-corrected estimates."
180,Convolutional Neural Network,A neural network where not all lawyers between the input and output layers are connected because not all units are connected. This decreases parameters and the number of parameters that need to be learned.,"Manon Legrand, Deep Reinforcement Learning for Autonomous Vehicle Control among Human Drivers, Universite Libre de Bruxelles, (2017).","In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, and natural language processing."
181,Stochastic Gradient Descent,A random method of iteratively optimizing a minimum value.,"Richard S. Sutton, Andrew G. Barto, Reinforcement Learning: An Introduction (MIT Press 2017).","Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization. A recent article implicitly credits Herbert Robbins and Sutton Monro for developing SGD in their 1951 article titled ""A Stochastic Approximation Method""; see Stochastic approximation for more information. It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set."
182,Internet Protocol Address,Numerical label assigned to each device connected to a computer network.,"Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication. An IP address serves two principal functions: host or network interface identification and location addressing. Internet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. However, because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP (IPv6), using 128 bits for the IP address, was developed in 1995, and standardized in December 1998. In July 2017, a final definition of the protocol was published. IPv6 deployment has been ongoing since the mid-2000s. The size of the routing prefix of the address is designated in CIDR notation by suffixing the address with the number of significant bits, which is equivalent to the historically used subnet mask 255.255.255.0. The IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIRs) responsible in their designated territories for assignment to end users and local Internet registries, such as Internet service providers. IPv4 addresses have been distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each. Each ISP or private network administrator assigns an IP address to each device connected to its network. Such assignments may be on a static (fixed or permanent) or dynamic basis, depending on its software and practices."
183,Network Host,"A network host is a network node that is assigned a network address and may offer information resources, services, and applications to users or other nodes on the network.","Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","A network host is a computer or other device connected to a computer network. A network host may offer information resources, services, and applications to users or other nodes on the network. A network host is a network node that is assigned a network address. Computers participating in networks that use the Internet protocol suite may also be called IP hosts. Specifically, computers participating in the Internet are called Internet hosts, sometimes Internet nodes. Internet hosts and other IP hosts have one or more IP addresses assigned to their network interfaces. The addresses are configured either manually by an administrator, automatically at startup by means of the Dynamic Host Configuration Protocol (DHCP), or by stateless address auto-configuration methods. Network hosts that participate in applications that use the client-server model of computing, are classified as server or client systems. Network hosts may also function as nodes in peer-to-peer applications, in which all nodes share and consume resources in an equipotent manner."
184,Internet Protocol Suite,The conceptual model and set of communications protocols used on the internet and similar computer networks.,"Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","The Internet protocol suite is the conceptual model and set of communications protocols used on the Internet and similar computer networks. It is commonly known as TCP/IP because the foundational protocols in the suite are the Transmission Control Protocol (TCP) and the Internet Protocol (IP). It is occasionally known as the Department of Defense (DoD) model because the development of the networking method was funded by the United States Department of Defense through DARPA. The Internet protocol suite provides end-to-end data communication specifying how data should be packetized, addressed, transmitted, routed, and received. This functionality is organized into four abstraction layers, which classify all related protocols according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication methods for data that remains within a single network segment (link); the internet layer, providing internetworking between independent networks; the transport layer, handling host-to-host communication; and the application layer, providing process-to-process data exchange for applications. Technical standards specifying the Internet protocol suite and many of its constituent protocols are maintained by the Internet Engineering Task Force (IETF). The Internet protocol suite predates the OSI model, a more comprehensive reference framework for general networking systems."
185,Data Availability,The accessibility of information in relation to time.,"Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","A data center (American English) is a dedicated space used to house computer systems and associated components, such as telecommunications and storage systems. It generally includes redundant or backup components and infrastructure for power supply, data communications connections, environmental controls and various security devices. A large data center is an industrial-scale operation using as much electricity as a small town."
186,Data Confidentiality,The availability of information in relation to the user.,"Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","Information security, sometimes shortened to InfoSec, is the practice of preventing unauthorized access, use, disclosure, disruption, modification, inspection, recording or destruction of information. The information or data may take any form, e.g. electronic or physical. Information security's primary focus is the balanced protection of the confidentiality, integrity and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a multi-step risk management process that identifies assets, threat sources, vulnerabilities, potential impacts, and possible controls, followed by assessment of the effectiveness of the risk management plan. To standardize this discipline, academics and professionals collaborate and seek to set basic guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability and user/administrator training standards. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, and transferred. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted."
187,Data Integrity,The accuracy of the information in relation to its intended use.,"Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","Data integrity is the maintenance of, and the assurance of the accuracy and consistency of, data over its entire life-cycle, and is a critical aspect to the design, implementation and usage of any system which stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context – even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a pre-requisite for data integrity. Data integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities,) and upon later retrieval, ensure the data is the same as it was when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties. Any unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security. Depending on the data involved this could manifest itself as benign as a single pixel in an image appearing a different color than was originally recorded, to the loss of vacation pictures or a business-critical database, to even catastrophic loss of human life in a life-critical system."
188,CIA Triad,"Term describing data confidentiality, data integrity, data availability.","Nayan B. Ruparelia, Cloud Computing (MIT Press, 2016).","The CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad—confidentiality, integrity and availability—are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) However, debate continues about whether or not this CIA triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy. Other principles such as ""accountability"" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts. In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment. Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security proposed 33 principles. From each of these derived guidelines and practices."
189,Point Cloud,A representation of data points in multi-dimensional space.,"Gaetan Pennecot, et. al., Devices and Methods For a Rotating LIDAR Platform With Shared Transmit/Receive Path, Google, Inc., No: 13/971,606, (Aug. 20, 2013).","A point cloud is a set of data points in space. Point clouds are generally produced by 3D scanners, which measure a large number of points on the external surfaces of objects around them. As the output of 3D scanning processes, point clouds are used for many purposes, including to create 3D CAD models for manufactured parts, for metrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications."
190,LIDAR,LIDAR is a type of optical radar sensor consisting of a transmitter and a receiver.,"Matthew J. McGill, LIDAR Remote Sensing, NASA Technical Reports Server (NTRS) (2002).","Lidar (also called LIDAR, LiDAR, and LADAR) is a surveying method that measures distance to a target by illuminating the target with pulsed laser light and measuring the reflected pulses with a sensor. Differences in laser return times and wavelengths can then be used to make digital 3-D representations of the target. The name lidar, now used as an acronym of light detection and ranging (sometimes light imaging, detection, and ranging), was originally a portmanteau of light and radar.  Lidar sometimes is called 3D laser scanning, a special combination of a 3D scanning and laser scanning. It has terrestrial, airborne, and mobile applications. Lidar is commonly used to make high-resolution maps, with applications in geodesy, geomatics, archaeology, geography, geology, geomorphology, seismology, forestry, atmospheric physics, laser guidance, airborne laser swath mapping (ALSM), and laser altimetry. The technology is also used in control and navigation for some autonomous cars."
191,Deep Learning,Allows machines to learn with architectures inspired by the biological neocortex.,"Ray Kurzweil, How to Create a Mind (Penguin Books 2012).","Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts. Deep learning models are vaguely inspired by information processing and communication patterns in biological nervous systems yet have various differences from the structural and functional properties of biological brains (especially human brains), which make them incompatible with neuroscience evidences."
192,On-Policy Algorithm,Directly optimize objective without using data.,"OpenAI, Spinning Up, User Documentation (2018) https://spinningup.openai.com/en/latest/user/introduction.html.","Vanilla Policy Gradient is the most basic, entry-level algorithm in the deep RL space because it completely predates the advent of deep RL altogether. The core elements of VPG go all the way back to the late 80s / early 90s. It started a trail of research which ultimately led to stronger algorithms such as TRPO and then PPO soon after. A key feature of this line of work is that all of these algorithms are on-policy: that is, they don’t use old data, which makes them weaker on sample efficiency. But this is for a good reason: these algorithms directly optimize the objective you care about—policy performance—and it works out mathematically that you need on-policy data to calculate the updates. So, this family of algorithms trades off sample efficiency in favor of stability—but you can see the progression of techniques (from VPG to TRPO to PPO) working to make up the deficit on sample efficiency."
193,Off-Policy Algorithm,Optimize objective with data using Bellman Equation.,"OpenAI, Spinning Up, User Documentation (2018) https://spinningup.openai.com/en/latest/user/introduction.html.","DDPG is a similarly foundational algorithm to VPG, although much younger—the theory of deterministic policy gradients, which led to DDPG, wasn’t published until 2014. DDPG is closely connected to Q-learning algorithms, and it concurrently learns a Q-function and a policy which are updated to improve each other. Algorithms like DDPG and Q-Learning are off-policy, so they are able to reuse old data very efficiently. They gain this benefit by exploiting Bellman’s equations for optimality, which a Q-function can be trained to satisfy using any environment interaction data (as long as there’s enough experience from the high-reward areas in the environment). But problematically, there are no guarantees that doing a good job of satisfying Bellman’s equations leads to having great policy performance. Empirically one can get great performance—and when it happens, the sample efficiency is wonderful—but the absence of guarantees makes algorithms in this class potentially brittle and unstable. TD3 and SAC are descendants of DDPG which make use of a variety of insights to mitigate these issues."
194,Intelligence Explosion,A hypothesized even in which an AI repidly improves from relatively modest to a superhuman level of intelligence.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","The intelligence explosion is a possible outcome of humanity building artificial general intelligence (AGI). AGI would be capable of recursive self-improvement leading to rapid emergence of ASI (artificial superintelligence), the limits of which are unknown, at the time of the technological singularity. I. J. Good speculated in 1965 that artificial general intelligence might bring about an intelligence explosion. He speculated on the effects of superhuman machines, should they ever be invented: Good (1965). Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. Good's scenario runs as follows: as computers increase in power, it becomes possible for people to build a machine that is more intelligent than humanity; this superhuman intelligence possesses greater problem-solving and inventive skills than current humans are capable of. This superintelligent machine then designs an even more capable machine, or re-writes its own software to become even more intelligent; this (even more capable) machine then goes on to design a machine of yet greater capability, and so on. These iterations of recursive self-improvement accelerate, allowing enormous qualitative change before any upper limits imposed by the laws of physics or theoretical computation set in."
195,Anthropomorphism,The ascription of human characteristics to non-human entities.,"The Perils & Promises of Artificial General Intelligence, 45 J. Legis. __ (2019) (Forthcoming).","Indeed, Ray Kurzweil argues that the twenty-first century will yield what today may seem like 20,000 years of technological progress and innovation because of the LOAR.  Additionally, Nick Bostrom and AI theorist Eliezer Yudkowsky have predicted a public perception of rapid kinetics in AI development due to human anthropomorphism of AI.  Human anthropomorphism of AI refers to the ascription of human levels of intelligence to non-human entities.  So, humans may consider a village idiot and Albert Einstein extreme ends of the intelligence spectrum.  Yet, the difference between the two on a larger relative scale is actually de minimis.  Thus, the advancement of an AI system from the intelligence of the village idiot, to the intelligence of Einstein, to the intelligence of AGI, and finally superintelligence may be faster than expected."
196,Optimization Power,The amount of quality-adjusted design effort being applied to improve a system's intelligence.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","A power optimizer is a DC to DC converter technology developed to maximize the energy harvest from solar photovoltaic or wind turbine systems. They do this by individually tuning the performance of the panel or wind turbine through maximum power point tracking, and optionally tuning the output to match the performance of the string inverter. Power optimizers are especially useful when the performance of the power generating components in a distributed system will vary widely, such as due to differences in equipment, shading of light or wind, or being installed facing different directions or widely separated locations. Power optimizers for solar applications can be similar to microinverters in that both systems attempt to isolate individual panels in order to improve overall system performance. A smart module is a power optimizer integrated into a solar module. A microinverter essentially combines a power optimizer with a small inverter in a single enclosure that is used on every panel, while the power optimizer leaves the inverter in a separate box and uses only one inverter for the entire array. The claimed advantage to this ""hybrid"" approach is lower overall system costs, avoiding the distribution of electronics."
197,Seed AI,An AI of initially modest capabilities that can bootstrap into an impressive AI by improving its own architecture.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. ""Superintelligence"" may also refer to the form or degree of intelligence possessed by such an agent. John von Neumann, Vernor Vinge and Ray Kurzweil define the concept in terms of the technological creation of super intelligence. They argue that it is difficult or impossible for present-day humans to predict what human beings' lives would be like in a post-singularity world. Technology forecasters and researchers disagree about if or when human intelligence is likely to be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of futures studies scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification."
198,Value Learning,An approach to the value-loading problem in which the AI learns the values humans want to pursue.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","Learning is the process of acquiring new, or modifying existing, knowledge, behaviors, skills, values, or preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in some plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulates from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be ""lost"" from that which cannot be retrieved. Human learning begins before birth and continues until death as a consequence of ongoing interactions between person and environment. The nature and processes involved in learning are studied in many fields, including educational psychology, neuropsychology, experimental psychology, and pedagogy. Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event can't be avoided nor escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development. Play has been approached by several theorists as the first form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games."
199,Whole Brain Emulation,Machine intelligence created by copying the computational structure of the human brain.,"Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford University Press 2017).","Whole brain emulation (WBE), mind upload or brain upload (sometimes called ""mind copying"" or ""mind transfer"") is the hypothetical futuristic process of scanning the mental state (including long-term memory and ""self"") of a particular brain substrate and copying it to a computer. The computer could then run a simulation model of the brain's information processing, such that it responds in essentially the same way as the original brain (i.e., indistinguishable from the brain for all relevant purposes) and experiences having a conscious mind. Mind uploading may potentially be accomplished by either of two methods: Copy-and-transfer or gradual replacement of neurons. In the case of the former method, mind uploading would be achieved by scanning and mapping the salient features of a biological brain, and then by copying, transferring, and storing that information state into a computer system or another computational device. The biological brain may not survive the copying process. The simulated mind could be within a virtual reality or simulated world, supported by an anatomic 3D body simulation model. Alternatively the simulated mind could reside in a computer that is inside (or connected to) a (not necessarily humanoid) robot or a biological body. Among some futurists and within the transhumanist movement, mind uploading is treated as an important proposed life extension technology. Some believe mind uploading is humanity's current best option for preserving the identity of the species, as opposed to cryonics. Another aim of mind uploading is to provide a permanent backup to our ""mind-file"", to enable interstellar space travels, and a means for human culture to survive a global disaster by making a functional copy of a human society in a Matrioshka brain, i.e. a computing device that consumes all energy from a star. Whole brain emulation is discussed by some futurists as a ""logical endpoint"" of the topical computational neuroscience and neuroinformatics fields, both about brain simulation for medical research purposes. It is discussed in artificial intelligence research publications as an approach to strong AI. Computer-based intelligence such as an upload could think much faster than a biological human even if it were no more intelligent. A large-scale society of uploads might, according to futurists, give rise to a technological singularity, meaning a sudden time constant decrease in the exponential development of technology. Mind uploading is a central conceptual feature of numerous science fiction novels and films. Substantial mainstream research in related areas is being conducted in animal brain mapping and simulation, development of faster supercomputers, virtual reality, brain–computer interfaces, connectomics and information extraction from dynamically functioning brains.[6] According to supporters, many of the tools and ideas needed to achieve mind uploading already exist or are currently under active development; however, they will admit that others are, as yet, very speculative, but still in the realm of engineering possibility. Neuroscientist Randal Koene has formed a nonprofit organization called Carbon Copies to promote mind uploading research."
200,Field,Something existing throughout space-time as opposed to a particle at one point in space-time.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","In mathematics, a field is a set on which addition, subtraction, multiplication, and division are defined, and behave as the corresponding operations on rational and real numbers do. A field is thus a fundamental algebraic structure, which is widely used in algebra, number theory and many other areas of mathematics. The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements. The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle can not be done with a compass and straightedge. Moreover, it shows that quintic equations are algebraically unsolvable. Fields serve as foundational notions in several mathematical domains. This includes different branches of analysis, which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the scalars for a vector space, which is the standard general context for linear algebra. Number fields, the siblings of the field of rational numbers, are studied in depth in number theory. Function fields can help describe properties of geometric objects."
201,Einstein-Rosen Bridge,A thin tube of space-time linking two black holes.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","The equations of the theory of general relativity have valid solutions that contain wormholes. The first type of wormhole solution discovered was the Schwarzschild wormhole, which would be present in the Schwarzschild metric describing an eternal black hole, but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them. An artist's impression of a wormhole from an observer's perspective, crossing the event horizon of a Schwarzschild wormhole that bridges two different universes. The observer originates from the right, and another universe becomes visible in the center of the wormhole's shadow once the horizon is crossed, the observer seeing light that has fallen into the black hole interior region from the other universe; however, this other universe is unreachable in the case of a Schwarzschild wormhole, as the bridge always collapses before the observer has time to cross it, and everything that has fallen through the event horizon of either universe is inevitably crushed in the singularity. Schwarzschild wormholes, also known as Einstein–Rosen bridges (named after Albert Einstein and Nathan Rosen), are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, ""maximally extended"" refers to the idea that the spacetime should not have any ""edges"": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a geodesic in the spacetime). In order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up away from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different ""universes"", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates. In this spacetime, it is possible to come up with coordinate systems such that if a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') is picked and an ""embedding diagram"" drawn depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an ""Einstein–Rosen bridge"". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe. The Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John Archibald Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region. According to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity.[clarification needed] Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge. Although Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the ""throat"" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy). Other non-traversable wormholes include Lorentzian wormholes (first proposed by John Archibald Wheeler in 1957), wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold, and Euclidean wormholes (named after Euclidean manifold, a structure of Riemannian manifold)."
202,Wave Frequency,The number of complete cycles per second.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Frequency is the number of occurrences of a repeating event per unit of time. It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency. For example: if a newborn baby's heart beats at a frequency of 120 times a minute, its period—the time interval between beats—is half a second (60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light."
203,Atom,"The basic unit off ordinary matter, made of a nucleus surrounded by orbiting electrons.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Frequency is the number of occurrences of a repeating event per unit of time. It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency. For example: if a newborn baby's heart beats at a frequency of 120 times a minute, its period—the time interval between beats—is half a second (60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light. An atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small; typical sizes are around 100 picometers (a ten-billionth of a meter, in the short scale). Atoms are small enough that attempting to predict their behavior using classical physics – as if they were billiard balls, for example – gives noticeably incorrect predictions due to quantum effects. Through the development of physics, atomic models have incorporated quantum principles to better explain and predict this behavior. Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and typically a similar number of neutrons. Protons and neutrons are called nucleons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, that atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively, and it is called an ion. The electrons of an atom are attracted to the protons in an atomic nucleus by this electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by a different force, the nuclear force, which is usually stronger than the electromagnetic force repelling the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force, and nucleons can be ejected from the nucleus, leaving behind a different element: nuclear decay resulting in nuclear transmutation. The number of protons in the nucleus defines to what chemical element the atom belongs: for example, all copper atoms contain 29 protons. The number of neutrons defines the isotope of the element. The number of electrons influences the magnetic properties of an atom. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature and is the subject of the discipline of chemistry."
204,Duality,A correspondence between apparently different theories leading to the same result.,"Stephen Hawking, A Brief History of Time (Bantam Books 1996).","In mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of A is B, then the dual of B is A. Such involutions sometimes have fixed points, so that the dual of A is A itself. For example, Desargues' theorem is self-dual in this sense under the standard duality in projective geometry. In mathematical contexts, duality has numerous meanings although it is ""a very pervasive and important concept in (modern) mathematics"" and ""an important general theme that has manifestations in almost every area of mathematics"". Many mathematical dualities between objects of two types correspond to pairings, bilinear functions from an object of one type and another object of the second type to some family of scalars. For instance, linear algebra duality corresponds in this way to bilinear maps from pairs of vector spaces to scalars, the duality between distributions and the associated test functions corresponds to the pairing in which one integrates a distribution against a test function, and Poincaré duality corresponds similarly to intersection number, viewed as a pairing between submanifolds of a given manifold."
205,Grand Unified Theory,"A theory which unifies the electromagnetic, strong, and weak forces.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Pursuing a bottom-up approach to explore which flavor symmetry could serve as an explanation of the observed fermion masses and mixings, we discuss an extension of the standard model (SM) where the flavor structure for both quarks and leptons is determined by a spontaneously broken S4 and the requirement that its particle content is embeddable simultaneously into the conventional SO(10) grand unified theory (GUT) and a continuous flavor symmetry Gf like SO(3)f or SU(3)f. We explicitly provide the Yukawa and the Higgs sector of the model and show its viability in two numerical examples which arise as small deviations from rank one matrices. In the first case, the corresponding mass matrix is democratic and in the second one only its 2-3 block is non-vanishing. We demonstrate that the Higgs potential allows for the appropriate vacuum expectation value (VEV) configurations in both cases, if CP is conserved. For the first case, the chosen Yukawa couplings can be made natural by invoking an auxiliary Z2 symmetry. The numerical study we perform shows that the best-fit values for the lepton mixing angles θ12 and θ23 can be accommodated for normal neutrino mass hierarchy. The results for the quark mixing angles turn out to be too small. Furthermore the CP-violating phase δ can only be reproduced correctly in one of the examples. The small mixing angle values are likely to be brought into the experimentally allowed ranges by including radiative corrections. Interestingly, due to the S4 symmetry the mass matrix of the right-handed neutrinos is proportional to the unit matrix. A Grand Unified Theory (GUT) is a model in particle physics in which, at high energy, the three gauge interactions of the Standard Model which define the electromagnetic, weak, and strong interactions, or forces, are merged into one single force. This unified interaction is characterized by one larger gauge symmetry and thus several force carriers, but one unified coupling constant. If Grand Unification is realized in nature, there is the possibility of a grand unification epoch in the early universe in which the fundamental forces are not yet distinct. Models that do not unify all interactions using one simple group as the gauge symmetry, but do so using semisimple groups, can exhibit similar properties and are sometimes referred to as Grand Unified Theories as well. Unifying gravity with the other three interactions would provide a theory of everything (TOE), rather than a GUT. Nevertheless, GUTs are often seen as an intermediate step towards a TOE. The novel particles predicted by GUT models are expected to have masses around the GUT scale—just a few orders of magnitude below the Planck scale—and so will be well beyond the reach of any foreseen particle collider experiments. Therefore, the particles predicted by GUT models will be unable to be observed directly and instead the effects of grand unification might be detected through indirect observations such as proton decay, electric dipole moments of elementary particles, or the properties of neutrinos. Some GUTs, such as the Pati-Salam model, predict the existence of magnetic monopoles. GUT models which aim to be completely realistic are quite complicated, even compared to the Standard Model, because they need to introduce additional fields and interactions, or even additional dimensions of space. The main reason for this complexity lies in the difficulty of reproducing the observed fermion masses and mixing angles which may be related to an existence of some additional family symmetries beyond the conventional GUT models. Due to this difficulty, and due to the lack of any observed effect of grand unification so far, there is no generally accepted GUT model."
206,Red Shift,"The reddening of light from a star moving away, dut to the Doppler effect.","Stephen Hawking, A Brief History of Time (Bantam Books 1996).","Where E is the energy of the y ray, Iis the nuclear mass, and k is Boltzmann's constant. The factor (E 2/2Mc'k8D) is the ratio of the recoil energy that would be taken up by the free nucleus to kea. For y rays much above the 129 kev employed by Mossbauer the factor f becomes very small even at absolute zero. The most striking evidence for the existence of this effect is the observation that the attenuation of the 129-kev gamma rays of Ir'9' in passing through an iridium absorber is reduced if the source is moved. The speed required to reduce the part of the attenuation caused by resonant scattering to one-half its maximum value was found to be approximately 1.5 cm/sec. From this a half-life of the excited state is derived to be 0.1 my, sec. Others have repeated this experiment, and extended it to helium temperatures. '»' One other case is reported, that of W' ' wherein a half-life of 0.6 my sec is inferred by the Doppler width of the resonance. This is half the accepted lifetime as measured by delay coincidence techniques. It is not clear whether this discrepancy represents a limit of the technique or whether it is largely an instrumental problem, as the authors suggest, enhanced by the complex array of other y rays in the Ta""' source. Of course, as has been suggested, one should expect to see effects caused by hyperfine structure in these spectra when lifetimes are long enough to allow them to be important. All the effects discussed in connection with the directional correlation of cascade y rays should have an influence. For example, it mould seem desirable to use a source that has a good chance of being in a normal lattice site and electronic state at the time of emission of the final y ray in question. One could have serious aftereffects from P decays, from prior emission of high-energy y rays, or from electron captures as well as broadening from imperfections in the crystal lattice or short spin-lattice relaxation. Even if the further development of the technique does not yield still narrower resonances, those already observed have fractional widths in frequency well below those of all the reference lines yet proposed for ""atomic clocks. "" If the scattering is reduced to one-half its maximum by relative motion of the source and scatterer with velocity v, the Q, the ratio of the frequency to the full width at half-height of the resonance line being observed, is just c/2v. In the case of Mossbauer's experiment Q is about 1 x10"" and in the case of W jt is 7x10~ . A measurement of the gravitational red shift could be performed by transmitting y rays from a source to a scatterer at an altitude different by h and by observing what relative velocity yields maximum scattering. For the predicted shift to be a full half-width of the line, the altitude difference h must be hz, = [4.18/E&(Mev) i~,(mpsec)] km. Thus, for the width reported for W'8', 66 km difference of height would be required. It is exciting to speculate about the possibilities opened up if cases of even less breadth can be found. For example, Fe"", for which 8& =0.0144 Mev and Tz, -—100 m psec, would require only 2.9 km separation were it to yield its natural breadth. Another example might be Zne' with an excited level at 0.093 Mev, of half-life 9400 m @sec. For this, if the natural breadth were obtained, h» would be 4.74 meters. This possibility represents a considerable extrapolation from present data. We are undertaking to examine these and other isotopes in various environments with the aim of selecting an isotope suitable for a gravitational experiment. Among other things equivalence of or absence of hypgrfine structures in the sources and scatterers would be desirable. Obviously one of the difficulties with large separations between source and scatter er ar ises from the inverse-square law of intensity. As a consequence of the participation of a large number of identical nuclei in an individual recoilfree scattering process, one anticipates the existence of intense Bragg diffraction from thin crystals. Thus one has the possibility of some. In physics, redshift is a phenomenon where electromagnetic radiation (such as light) from an object undergoes an increase in wavelength. Whether or not the radiation is visible, ""redshift"" means an increase in wavelength, equivalent to a decrease in wave frequency and photon energy, in accordance with, respectively, the wave and quantum theories of light. The emitted light is not actually red; instead, the term refers to the human perception of longer wavelengths at the red end of the visible spectrum. Examples of redshifting are a gamma ray perceived as an X-ray, or initially visible light perceived as radio waves. The opposite of a redshift is a blueshift, where wavelengths shorten and energy increases. However, redshift is a more common term and sometimes blueshift is referred to as negative redshift. There are three main causes of red (and blue shifts) in astronomy and cosmology: Objects move apart (or closer together) in space. This is an example of the Doppler effect. Space itself expands, causing objects to become separated without changing their positions in space. This is known as cosmological redshift. All sufficiently distant light sources (generally more than a few million light years away) show redshift corresponding to the rate of increase in their distance from Earth, known as Hubble's Law. Gravitational redshift is a relativistic effect observed due to strong gravitational fields, which distort spacetime and exert a force on light and other particles. Knowledge of redshifts and blueshifts has been used to develop several terrestrial technologies such as Doppler radar and radar guns. Redshifts are also seen in the spectroscopic observations of astronomical objects. Its value is represented by the letter z. A special relativistic redshift formula (and its classical approximation) can be used to calculate the redshift of a nearby object when spacetime is flat. However, in many contexts, such as black holes and Big Bang cosmology, redshifts must be calculated using general relativity. Special relativistic, gravitational, and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from true redshift and are not generally referred to as such (see section on physical optics and radiative transfer)."
208,Dot Product,Takes two equal sequences of vector values and returns a single scalar value.,"Khan Academy,Vector Dot Product and Vector Length,  https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length.","In mathematics, the dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used and often called ""the"" inner product (or rarely projection product) of Euclidean space even though it is not the only inner product that can be defined on Euclidean space; see also inner product space. Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry, Euclidean spaces are often defined by using vector spaces. In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths). The name ""dot product"" is derived from the centered dot "" · "" that is often used to designate this operation; the alternative name ""scalar product"" emphasizes that the result is a scalar, rather than a vector, as is the case for the vector product in three-dimensional space."
209,Real Number,A value of a point on a line.,"What are the “real numbers” really?, https://math.vanderbilt.edu/schectex/courses/thereals/.","The short, simple answer used in calculus courses is that a real number is a point on the number line. That's not the whole truth, but it is adequate for the needs of freshman calculus. The freshman calculus course (at most universities nowadays) follows the 17th century style of Newton and Leibniz, emphasizing computations and omitting many proofs. The omitted proofs depend on a careful explanation of what the ""real numbers"" really are. That explanation and those proofs were not discovered until the 19th century, after Newton and Leibniz were long dead. A proper explanation of the real numbers nowadays is covered, if at all, in a course in ""real analysis"" in the junior or senior year of students who are majoring in mathematics. Surprisingly few students take such a course; perhaps that's because it is too algebraic for the analysts' taste and too analytic to please the algebraists. In this web page, I'll discuss the mathematical meaning of ""real number."" Before that, I want to discuss this more elementary question: where did the name ""real"" come from? (It turns out to have little to do with the deeper properties of real numbers.) To answer that question, I first need to talk about complex numbers."
210,Kernel,A small square matrix applied to each element of an input matrix.,"Manon Legrand, Deep Reinforcement Learning for Autonomous Vehicle Control among Human Drivers, Universite Libre de Bruxelles, 24 (2017).","Indeed, a convolutional neural network (“CNN”) is a type of DNN that is used in this context. CNNs are modeled based upon the biological visual cortex. The biological visual cortex is composed of receptive fields made up of cells that are sensitive to small sub-regions of the visual field. In an artificial visual cortex, the response of a neuron to a stimulus in its receptive field is modeled with a mathematical convolutional operation. Convolution is a form of mathematical operation with two matrices: an input matrix and a kernel, also called a filter. A kernel is a small square matrix that is applied each element of the input matrix. Applied to AVs, the input of a CNN is often 3D LIDAR data and the output is a mapping of an AV’s environment that allows an AV to identify objects in its environment and to ascribe meaning to those objects through relativistic associations. The function of the CNN is in essence a classification task, where the CNN classifies objects based upon their similarity. The model learns through both supervised and unsupervised learning processes and is designed to capture human intuition in visual classifications."
211,Pooling,A process of combining values of neurons in previous layer with a neuron in a later layer.,"Convolutional Neural Networks, Stanford, http://cs231n.github.io/convolutional-networks/#pool.","It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice)."
212,Max-Pooling,A process of pooling using the maximum value of the neurons in the previous layer.,"Convolutional Neural Networks, Stanford, http://cs231n.github.io/convolutional-networks/#pool.","The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2 (i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance. Reducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired."
213,Convolution,A form of mathematical operation with two matrices: an input matrix and a kernel.,"Haney, Brian Seamus, The Optimal Agent: The Future of Autonomous Vehicles & Liability Theory (October 5, 2018). Available at SSRN: https://ssrn.com/abstract=3261275 or http://dx.doi.org/10.2139/ssrn.3261275.","CNNs are modeled based upon the biological visual cortex. The biological visual cortex is composed of receptive fields made up of cells that are sensitive to small sub-regions of the visual field. In an artificial visual cortex, the response of a neuron to a stimulus in its receptive field is modeled with a mathematical convolutional operation. Convolution is a form of mathematical operation with two matrices: an input matrix and a kernel, also called a filter. A kernel is a small square matrix that is applied each element of the input matrix. Applied to AVs, the input of a CNN is often 3D LIDAR data and the output is a mapping of an AV’s environment that allows an AV to identify objects in its environment and to ascribe meaning to those objects through relativistic associations. The function of the CNN is in essence a classification task, where the CNN classifies objects based upon their similarity. The model learns through both supervised and unsupervised learning processes and is designed to capture human intuition in visual classifications."
214,Graphics Processing Unit,Electronic circuit designed to rapidly manipulate memory to accelerate the creation of images.,"Google Cloud, Graphics Processing Unit (GPU), https://cloud.google.com/gpu/.","A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose CPUs for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In certain CPUs, they are embedded on the CPU die. The term GPU has been used from at least the 1980s, it was popularized by Nvidia in 1999, who marketed the GeForce 256 as ""the world's first GPU"". It was presented as a ""single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines"". Rival ATI Technologies coined the term ""visual processing unit"" or VPU with the release of the Radeon 9700 in 2002."
215,Softmax,A function mapping input to a probability distribution.,"Uniqtech Co., Understand the Softmax Function in Minutes, Medium, https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d.","The name ""softmax"" is misleading; the function is not a smooth maximum (a smooth approximation to the maximum function), but is rather a smooth approximation to the arg max function: the function whose value is which index has the maximum. In fact, the term ""softmax"" is also used for the closely related LogSumExp function, which is a smooth maximum. For this reason, some prefer the more accurate term ""softargmax"", but the term ""softmax"" is conventional in machine learning. For this section, the term ""softargmax"" is used to emphasize this interpretation."
216,Derivative,Measures the rate of change of a function over time.,"Formal Definition of The Derivative as A Limit, Khan Academy, https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-2/v/calculus-derivatives-1-new-hd-version.","The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances. The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the ""instantaneous rate of change"", the ratio of the instantaneous change in the dependent variable to that of the independent variable. Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector."
217,Chain Rule,Formula for computing the derivative of two or more functions.,"Chain Rule, Khan Academy, https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction.","In the statement of the chain rule, f and g play slightly different roles because f′ is evaluated at g(t) whereas g′ is evaluated at t. This is necessary to make the units work out correctly. For example, suppose that we want to compute the rate of change in atmospheric pressure ten seconds after the skydiver jumps. This is (f ∘ g)′(10) and has units of pascals per second. The factor g′(10) in the chain rule is the velocity of the skydiver ten seconds after his jump, and it is expressed in meters per second. f′(g(10)) is the change in pressure with respect to height at the height g(10) and is expressed in pascals per meter. The product of f′(g(10)) and g′(10) therefore has the correct units of pascals per second. It is not possible to evaluate f anywhere else. For instance, because the 10 in the problem represents ten seconds, the expression f′(10) represents the change in pressure at a height of ten seconds, which is nonsense. Similarly, because g′(10) = −98 meters per second, the expression f′(g′(10)) represents the change in pressure at a height of −98 meters per second, which is also nonsense. However, g(10) is 3020 meters above sea level, the height of the skydiver ten seconds after his jump. This has the correct units for an input to f."
218,Loss Function,Measure of performance guiding the update of parameters.,"Gradient Descent, https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html.","In mathematical optimization, statistics, econometrics, decision theory, machine learning and computational neuroscience, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. In statistics, typically a loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century. In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss. In classical statistics (both frequentist and Bayesian), a loss function is typically treated as something of a background mathematical convention. Critics such as W. Edwards Deming and Nassim Nicholas Taleb have argued that loss functions require much greater attention than they have traditionally been given and that loss functions used in real world decision making need to reflect actual empirical experience. They argue that real-world loss functions are often very different from the smooth, symmetric ones used by classical convention, and are often highly asymmetric, nonlinear, and discontinuous."
219,Members,Properties of a class.,"Mike McGrath, Coding for Beginners (2017).","In object-oriented programming, a member variable (sometimes called a member field) is a variable that is associated with a specific object, and accessible for all its methods (member functions). In class-based languages, these are distinguished into two types: if there is only one copy of the variable shared with all instances of the class, it is called a class variable or static member variable; while if each instance of the class has its own copy of the variable, the variable is called an instance variable."
220,Methods,Function of a class.,"Mike McGrath, Coding for Beginners (2017).","A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object is mostly made up of data and behavior, which form the interface that an object presents to the outside world. Data is represented as properties of the object and behavior as methods. For example, a Window object would have methods such as open and close, while its state (whether it is opened or closed) would be a property. In class-based programming, methods are defined in a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding. The same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc. Methods also provide the interface that other classes use to access and modify the data properties of an object. This is known as encapsulation. Encapsulation and overriding are the two primary distinguishing features between methods and procedure calls."
221,Instance Variable,Variable declared with in method.,"Mike McGrath, Coding for Beginners (2017).","In object-oriented programming with classes, an instance variable is a variable defined in a class (i.e. a member variable ), for which each instantiated object of the class has a separate copy, or instance. An instance variable is similar to a class variable. An instance variable is a variable which is declared in a class but outside the constructor and the method/function. Instance variables are created when an object is instantiated, and are accessible to all the methods, the constructor and block in the class. Access modifiers can be given to the instance variable. An instance variable is not a class variable although there are similarities. It is a type of class attribute (or class property, field, or data member). The same dichotomy between instance and class members applies to methods (""member functions"") as well; a class may have both instance methods and class methods."
222,Encapsulation,Ensures data is stored within a class structure.,"Mike McGrath, Coding for Beginners (2017).","In object oriented programming languages, encapsulation is used to refer to one of two related but distinct notions, and sometimes to the combination thereof: A language mechanism for restricting direct access to some of the object's components. A language construct that facilitates the bundling of data with the methods (or other functions) operating on that data. Some programming language researchers and academics use the first meaning alone or in combination with the second as a distinguishing feature of object-oriented programming, while some programming languages that provide lexical closures view encapsulation as a feature of the language orthogonal to object orientation. The second definition is motivated by the fact that in many of the OOP languages hiding of components is not automatic or can be overridden; thus, information hiding is defined as a separate notion by those who prefer the second definition. The features of encapsulation are supported using classes in most object-oriented programming languages, although other alternatives also exist."
223,Parameter,Variables within a function.,"Mike McGrath, Coding for Beginners (2017).","Parameter has more specific meanings within various disciplines, including mathematics, computing and computer programming, engineering, statistics, logic and linguistics. Within and across these fields, careful distinction must be maintained of the different usages of the term parameter and of other terms often associated with it, such as argument, property, axiom, variable, function, attribute, etc."
224,Argument,Data value passed to function parameters,"Mike McGrath, Coding for Beginners (2017).","In logic and philosophy, an argument is a series of statements (in a natural language), called the premises or premisses (both spellings are acceptable), intended to determine the degree of truth of another statement, the conclusion. The logical form of an argument in a natural language can be represented in a symbolic formal language, and independently of natural language formally defined ""arguments"" can be made in math and computer science. Logic is the study of the forms of reasoning in arguments and the development of standards and criteria to evaluate arguments. Deductive arguments can be valid or sound: in a valid argument, premisses necessitate the conclusion, even if one or more of the premisses is false and the conclusion is false; in a sound argument, true premisses necessitate a true conclusion. Inductive arguments, by contrast, can have different degrees of logical strength: the stronger or more cogent the argument, the greater the probability that the conclusion is true, the weaker the argument, the lesser that probability. The standards for evaluating non-deductive arguments may rest on different or additional criteria than truth—for example, the persuasiveness of so-called ""indispensability claims"" in transcendental arguments, the quality of hypotheses in retroduction, or even the disclosure of new possibilities for thinking and acting."
225,Hash Function,A search algorithm mapping items and slots.,"Bradley N. Miller, David L. Ranum, Problem Solving with Algorithms and Data Structures Using Python (2nd. Franklin, Beedle & Associates 2011).","A hash function that maps names to integers from 0 to 15. There is a collision between keys ""John Smith"" and ""Sandra Dee"". A hash function is any function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. One such application is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or any equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication. Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The HashKeeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values."
226,Package,A collection of related modules.,"Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","A package manager or package management system is a collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer's operating system in a consistent manner. A package manager deals with packages, distributions of software and data in archive files. Packages contain metadata, such as the software's name, description of its purpose, version number, vendor, checksum, and a list of dependencies necessary for the software to run properly. Upon installation, metadata is stored in a local package database. Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. They work closely with software repositories, binary repository managers, and app stores. Package managers are designed to eliminate the need for manual installs and updates. This can be particularly useful for large enterprises whose operating systems are based on Linux and other Unix-like systems, typically consisting of hundreds or even tens of thousands of distinct software packages."
227,Module,A collection of variable and function definitions.,"Steven Bird, et. al., Natural Language Processing with Python, (O’Reily, 2009).","A computer module is a selection of independent electronic circuits packaged onto a circuit board to provide a basic function within a computer. An example might be an inverter or flip-flop, which would require two or more transistors and a small number of additional supporting devices. Modules would be inserted into a chassis and then wired together to produce a larger logic unit, like an adder."
